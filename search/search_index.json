{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Practices for CernVM-FS in HPC","text":"<p>Warning</p> <p>(Nov'23) This tutorial is under development, please come back later when the tutorial contents have been completed.</p> <p>An online version of this tutorial is planned for Mon 4 Dec 2023 (13:30-17:00 CET), register via https://event.ugent.be/registration/cvmfshpc202312.</p>"},{"location":"#scope","title":"Scope","text":"<p>This is an introductory tutorial to using CernVM-FS, the CernVM File System.</p> <p>In this tutorial you will learn what CernVM-FS is, how to get access to existing CernVM-FS repositories, and how to configure CernVM-FS on High-Performance Computing (HPC) infrastructure.</p>"},{"location":"#intended-audience","title":"Intended audience","text":"<p>This tutorial is intended for people with a background in HPC (system administrators, support team members, end users, etc.) who are new to CernVM-FS: no specific prior knowledge or experience with it is required.</p> <p>We expect it to be most valuable to people who are interested in using or providing access to one or more existing CernVM-FS repositories on HPC infrastructure.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>(more info soon)</p>"},{"location":"#practical-information","title":"Practical information","text":"<p>A first virtual edition of this tutorial is planned for Monday 4 December 2023 (13:30-17:00 CET).</p> <p>Attendance is free, but registration is required: https://event.ugent.be/registration/cvmfshpc202312.</p> <p>(more info soon)</p>"},{"location":"#tutorial-contents","title":"Tutorial contents","text":"<ul> <li>Introduction to CernVM-FS</li> <li>European Environment for Scientific Software (EESSI)</li> <li>Accessing a CernVM-FS repository</li> <li>Configuring CernVM-FS on HPC infrastructure</li> <li>Troubleshooting and debugging CernVM-FS</li> <li>Performance aspects of CernVM-FS</li> <li>Different storage backends for CernVM-FS</li> <li>Containers and CernVM-FS</li> <li>Getting started with CernVM-FS (from scratch)</li> <li>Appendix: Terminology</li> </ul> <p>(sections indicated with <code>(*)</code> involve hands-on exercises)</p>"},{"location":"#slides","title":"Slides","text":"<p>(coming soon)</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Jakob Blomer (CERN, Switzerland)</li> <li>Bob Dr\u00f6ge (University of Groningen, The Netherlands)</li> <li>Kenneth Hoste (HPC-UGent, Belgium)</li> <li>Alan O'Cais (University of Barcelona, Spain + CECAM)</li> <li>Lara Peeters (HPC-UGent, Belgium)</li> <li>Laura Promberger (CERN, Switzerland)</li> <li>Valentin V\u00f6lkl (CERN, Switzerland)</li> </ul>"},{"location":"#additional-resources","title":"Additional resources","text":"<ul> <li>CernVM-FS website</li> <li>CernVM-FS documentation</li> <li>CernVM-FS @ GitHub</li> <li>Introduction to CernVM-FS by Jakob Blomer (CERN) (2021)</li> <li>Introductory tutorial on CernVM-FS (2021)</li> </ul>"},{"location":"02_access/","title":"Accessing a CernVM-FS repository","text":""},{"location":"02_access/#client-configuration","title":"Client configuration","text":"<p>(minimal requirement)</p>"},{"location":"02_access/#squid-proxy","title":"Squid proxy","text":""},{"location":"02_access/#stratum-1-replicate-server","title":"Stratum-1 replicate server","text":"<p>(private)</p>"},{"location":"03_configuration/","title":"Configuring CernVM-FS on HPC infrastructure","text":""},{"location":"03_configuration/#diskless-workernodes","title":"Diskless workernodes","text":""},{"location":"03_configuration/#offline-workernodes","title":"Offline workernodes","text":""},{"location":"03_configuration/#alien-cache","title":"Alien cache","text":""},{"location":"03_configuration/#security","title":"Security","text":""},{"location":"03_configuration/#syncing-a-cernvm-fs-repository-to-another-filesystem","title":"Syncing a CernVM-FS repository to another filesystem","text":""},{"location":"04_troubleshooting_debugging/","title":"Troubleshooting and debugging CernVM-FS","text":""},{"location":"04_troubleshooting_debugging/#logs","title":"Logs","text":""},{"location":"04_troubleshooting_debugging/#stats","title":"Stats","text":""},{"location":"04_troubleshooting_debugging/#common-problems","title":"Common problems","text":""},{"location":"04_troubleshooting_debugging/#monitoring","title":"Monitoring","text":""},{"location":"04_troubleshooting_debugging/#mounting-in-debug-mode","title":"Mounting in debug mode","text":""},{"location":"05_performance/","title":"Performance aspects of CernVM-FS","text":""},{"location":"05_performance/#startup-performance","title":"Startup performance","text":""},{"location":"05_performance/#os-jitter-by-cernvm-fs-daemon","title":"OS jitter by CernVM-FS daemon","text":""},{"location":"05_performance/#using-a-cdn","title":"Using a CDN","text":""},{"location":"06_storage_backends/","title":"Different storage backends for CernVM-FS","text":""},{"location":"06_storage_backends/#s3","title":"S3","text":""},{"location":"06_storage_backends/#tradeoffs","title":"Tradeoffs","text":""},{"location":"07_containers/","title":"Containers and CernVM-FS","text":""},{"location":"07_containers/#accessing-a-cernvm-fs-repository-via-apptainer","title":"Accessing a CernVM-FS repository via Apptainer","text":""},{"location":"07_containers/#ingesting-container-images-in-a-cernvm-fs-repository","title":"Ingesting container images in a CernVM-FS repository","text":""},{"location":"10_getting_started/","title":"Getting started with CernVM-FS (from scratch)","text":""},{"location":"10_getting_started/#setting-up-the-stratum-0-server","title":"Setting up the Stratum-0 server","text":""},{"location":"10_getting_started/#creating-a-cernvm-fs-repository","title":"Creating a CernVM-FS repository","text":""},{"location":"10_getting_started/#setting-up-a-stratum-1-replica-server","title":"Setting up a Stratum-1 replica server","text":""},{"location":"eessi/","title":"European Environment for Scientific Software (EESSI)","text":""},{"location":"appendix/terminology/","title":"CernVM-FS Terminology","text":"<p>An overview of terms used in the context of CernVM-FS, in alphabetical order.</p>"},{"location":"appendix/terminology/#catalog","title":"Catalog","text":"<p>A catalog of a CernVM-FS repository is a table that lists files and directories along with the corresponding metadata (sizes, timestamps, etc.).</p> <p>Catalogs can be nested: subtrees of the repository may have their own catalog.</p> <p>For more information on the catalog concept, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#cernvm","title":"CernVM","text":"<p>CernVM is a virtual machine image based on CentOS combined with a custom, virtualization-friendly Linux kernel, and which includes the CernVM-FS client software.</p> <p>It is used for the CERN Large Hadron Collider (LHC) experiment, and was created to remove a need for the installation of the experiment software and to minimize the number of platforms (compiler-OS combinations) on which experiment software needs to be supported and tested.</p> <p>While originally developed in conjunction, the CernVM File System today is a product that is completely independent from the CernVM virtual appliance.</p> <p>For more information on CernVM, see the website and documentation.</p>"},{"location":"appendix/terminology/#cvmfs","title":"CernVM-FS","text":"<p>(see What is CernVM-FS?)</p>"},{"location":"appendix/terminology/#client","title":"Client","text":"<p>A client in the context of CernVM-FS is a computer system on which a CernVM-FS repository is being accessed, on which it will be presented as a POSIX read-only file system in a subdirectory of <code>/cvmfs</code>.</p>"},{"location":"appendix/terminology/#proxy","title":"Proxy","text":"<p>A proxy, also referred to as squid proxy, is a forward caching proxy server which acts as an intermediary between a CernVM-FS client and the Stratum-1 replica servers.</p> <p>It is used to improve the latency observed when accessing the contents of a repository, and to reduce the load on the Stratum-1 replica servers.</p> <p>A commonly used proxy is Squid.</p> <p>For more information on proxies, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#publishing","title":"Publishing","text":"<p>Publishing is the process of adding more files to a CernVM-FS repository, which is done via a transaction mechanism, and is on possible on the Stratum-0 server, via a publisher, or via a repository gateway.</p> <p>The workflow of publishing content is covered in detail in the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#repository","title":"Repository","text":"<p>A CernVM-FS repository is where the files and directories that you want to distribute via CernVM-FS are stored, which usually correspond to a collection of software installations.</p> <p>It is a form of content-addressable storage (CAS), and is the single source of (new) data for the file system being presented as a subdirectory of <code>/cvmfs</code> on client systems that mount the repository.</p> <p>Note</p> <p>A CernVM-FS repository includes software installations, not software packages like RPMs.</p>"},{"location":"appendix/terminology/#software-installations","title":"Software installations","text":"<p>An important distinction for a CernVM-FS repository compared to the more traditional notion of a software repository is that a CernVM-FS repository provides access to the individual files that collectively form a particular software installation, as opposed to housing a set of software packages like RPMs, each of which being a collection of files for a particular software installation that are packed together in a single package to distribute as a whole.</p> <p>Note</p> <p>This is an important distinction, since CernVM-FS enables only downloading the specific files that are required to perform a particular task with a software installation, which often is a small subset of all files that are part of that software installation.</p>"},{"location":"appendix/terminology/#stratum0","title":"Stratum 0 server","text":"<p>A Stratum 0 server, often simply referred to a Stratum 0 (Stratum Zero), is the central server for one or more CernVM-FS repositories.</p> <p>It is the single source of (new) data, since it hosts the master copy of the repository contents.</p> <p>Adding or updating files in a CernVM-FS repository (publishing) can only be done on the Stratum 0 server, either directly via the <code>cvmfs_server publish</code> command, or indirectory via a publisher server.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#stratum1","title":"Stratum 1 replica server","text":"<p>A Stratum 1 replica server, often simply referred to a Stratum 1 (Stratum One), is a standard web server that acts as a mirror server for one or more CernVM-FS repositories.</p> <p>It holds a complete copy of the data for each CernVM-FS repository it serves, and automatically synchronises with the Stratum 0.</p> <p>There is typically a network of several Stratum 1 servers for a CernVM-FS repository, which are geographically distributed.</p> <p>Clients can be configured to automatically connect to the closest Stratum 1 server by using the CernVM-FS GeoAPI.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"cvmfs/","title":"Introduction to CernVM-FS","text":"<ul> <li>What is CernVM-FS?</li> <li>Technical details</li> <li>Flagship repositories</li> </ul>"},{"location":"cvmfs/flagship-repositories/","title":"Flagship CernVM-FS repositories","text":""},{"location":"cvmfs/flagship-repositories/#lhc-experiments","title":"LHC experiments","text":"<p>CernVM-FS repositories are used to distribute the software required to analyse the data produced by the Large Hadron Collider (LHC) at each of the LHC experiments.</p> <p>Examples include (click to browse repository contents):</p> <ul> <li><code>/cvmfs/alice.cern.ch</code>: software for ALICE experiment</li> <li><code>/cvmfs/atlas.cern.ch</code>: software for ATLAS experiment</li> <li><code>/cvmfs/cms.cern.ch</code>: software for CMS experiment</li> <li><code>/cvmfs/lhcb.cern.ch</code>: software for LHCb experiment</li> <li><code>/cvmfs/sft.cern.ch</code>: LCG Software Stacks</li> </ul>"},{"location":"cvmfs/flagship-repositories/#the-alliance","title":"The Alliance","text":"<p>The Digital Research Alliance of Canada, a.k.a. The Alliance and formerly known as Compute Canada, uses CernVM-FS to distribute the software stack for the Canadian national compute clusters.</p> <p>Documentation on using their CernVM-FS repository <code>/cvmfs/soft.computecanada.ca</code> is available here, and an overview of available software is available here.</p>"},{"location":"cvmfs/flagship-repositories/#unpacked-containers","title":"Unpacked containers","text":"<p>CernVM-FS repositories can be used to provide an efficient way to access container images, by serving unpacked container images that can be consumed by container runtimes such as Apptainer.</p> <p>Examples include:</p> <ul> <li><code>/cvmfs/unpacked.cern.ch</code></li> <li><code>/cvmfs/singularity.opensciencegrid.org</code></li> </ul> <p>More information on <code>unpacked.cern.ch</code> is available in the CernVM-FS documentation:</p> <ul> <li>Container Images and CernVM-FS</li> <li>Working with DUCC and Docker Images</li> </ul>"},{"location":"cvmfs/flagship-repositories/#eessi","title":"EESSI","text":"<p>The European Environment for Scientific Software Installations (EESSI) provides optimized installations of scientific software for <code>x86_64</code> (Intel + AMD) and <code>aarch64</code> (64-bit Arm) systems that work on any Linux distribution.</p> <p>We will use EESSI as an example repository throughout this tutorial.</p> <p>More detailed information on EESSI is available in the next part of this tutorial.</p>"},{"location":"cvmfs/technical-details/","title":"Technical details of CernVM-FS","text":"<p>CernVM-FS is implemented as a POSIX read-only filesystem in user space (FUSE) with repositories of files that are served via outgoing HTTP connections only, thus avoiding problems with firewalls.</p> <p>Files in a CernVM-FS repository are automatically downloaded on-demand to a client system as they are accessed, from web servers that support the CernVM-FS repository being used.</p> <p>Internally, CernVM-FS uses content-adressable storage (CAS) and Merkle trees (like Git also does) to store file data and metadata.</p>"},{"location":"cvmfs/technical-details/#caching","title":"Caching","text":"<p>CernVM-FS uses a caching mechanism with a least-recently used (LRU) cache replacement policy, in which configurable local client cache is populated via either a forward proxy server (like Squid), or from a Stratum-1 replica server.</p> <p>Both the proxy and the replica server could be within the same local network as the client, or not.</p> <p>To help reduce performance problems regarding network latency and bandwidth, clients can leverage the Geo API supported by CernVM-FS Stratum-1 replica servers to automatically sort them geographically, in order to prioritize connecting to the closest ones.</p> <p>Furthermore, additional caches can be made available to CernVM-FS, such as an alien cache on a shared cluster filesystem like GPFS or Lustre that is not managed by CernVM-FS, and a Content Delivery Network (CDN) can be used to help limit the time required to download files that are not cached yet.</p>"},{"location":"cvmfs/what-is-cvmfs/","title":"What is CernVM-FS?","text":"<p>CernVM-FS, the CernVM File System (also known as CVMFS), is a file distribution service that is particularly well suited to distribute software installations across a large number of systems world-wide in an efficient way.</p> <p>From an end user perspective, files in a CernVM-FS repository are available read-only via a subdirectory in <code>/cvmfs</code>, with a user experience similar to that of an on-demand streaming service for music or video, but then (mainly) applied to software installations.</p>"},{"location":"cvmfs/what-is-cvmfs/#primary-use-case","title":"Primary use case","text":"<p>The primary use case of CernVM-FS is distributing software, and it provides several interesting features that support this, including:</p> <ul> <li>on-demand downloading and updating of repository contents;</li> <li>multi-level caching;</li> <li>de-duplication of files;</li> <li>compression of data;</li> <li>verification of data integrity;</li> </ul> <p>CernVM-FS has been proven to scale to billions of files and tens of thousands of clients.</p> <p>It was originally developed at CERN to let High Energy Physics (HEP) collaborations like the experiments at the Large Hadron Collider (LHC) deploy software on the Worldwide LHC Computing Grid (WLCG) infrastructure that is used to run data processing applications.</p> <p>The primary use case of distributing software is a particular one, since software often comprises many small files that are frequently opened and read as a whole, and frequent look-ups for files in multiple directories are triggered when search paths are examined.</p> <p>In certain cases, the CernVM-FS has also been used to distribute large data repositories.</p>"},{"location":"cvmfs/what-is-cvmfs/#features","title":"Features","text":""},{"location":"cvmfs/what-is-cvmfs/#features-ondemand","title":"On-demand downloading of files and metadata","text":"<p>The metadata and content of files included in a CernVM-FS repository are automatically downloaded on-demand as files and directories are being accessed, which is akin to streaming services for music, movies, and TV series.</p> <p>This happens fully transparently, as the contents of a repository are exposed by CernVM-FS as if it were a local (read-only) file system. Hence, clients that access a CernVM-FS repository typically do not actually have a local copy of all files included in that repository, but only have a limited set of files and metadata directly available: those which were most recently accessed.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-updating","title":"Automatic updates","text":"<p>CernVM-FS clients automatically pull in updates to the contents of a repository as they are published server-side. This happens in transactions, to ensure that clients observe a consistent state of the repository.</p> <p>Once a CernVM-FS repository is accessible on a client system, no subsequent actions must be taken to keep clients up-to-date other than updating CernVM-FS itself on a regular basis.</p> <p>This significantly limits the maintenance burden, since no action is required on client systems to update the software stack that is provided through a CernVM-FS repository, since the updates are streamed in automatically by CernVM-FS.</p> <p>Only the CernVM-FS client should be updated on a regular basis on client systems.</p> <p>For more elaborate setups that involve proxies or CernVM-FS replica (mirror) servers, additional maintenance is necessary, but again only to update the CernVM-FS components themselves.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-caching","title":"Multi-level caching","text":"<p>CernVM-FS uses a multi-level caching hierarchy to reduce the latency observed when accessing repository contents. Caching is an essential part of CernVM-FS, since the contents of a CernVM-FS repository are downloaded on-demand as they are accessed.</p> <p>The caching mechanism employed by CernVM-FS goes way beyond the standard (in-memory) Linux kernel file system cache, and consists of a local client cache, an optional forward proxy server that acts as an intermediary cache level, and a distributed network of mirror servers that support the CernVM-FS repository being accessed.</p> <p>When a part of the repository is being accessed that is not available yet in the local client cache, CernVM-FS will traverse the multi-level cache hierarchy to obtain the necessary data and update the local client cache with it, so the files being accessed can be served with low latency.</p> <p>Proxy and mirror servers scale horizontally: the CernVM-FS client makes automatic use of multiple deployed service instances for load-balancing and high-availability.</p> <p>We will explore this multi-level caching mechanism in more detail in this tutorial.</p> <p>See here more technical details on CernVM-FS caching.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-deduplication","title":"De-duplication of files","text":"<p>CernVM-FS stores the contents of a file only once, even when it is included multiple times in a particular repository at different paths.</p> <p>This can result in a significant reduction in storage capacity that is required to host a large software stack, especially when identical files are spread out across the repository, as often happens with particular files like example data files across multiple versions of the same software.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-compression","title":"Compression of data","text":"<p>CernVM-FS stores file content compressed on the server, which not only further reduces required storage space but also significantly limits the network bandwidth that is required to download (and serve) the contents of a repository.</p> <p>On the client side, the data is transparently decompressed when the files included in a CernVM-FS repository are presented under <code>/cvmfs</code> as a normal (read-only) file system.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-data-integrity","title":"Verification of data integrity","text":"<p>The integrity of data provided by a CernVM-FS server is ensured on a client system by verifying a cryptographic hash, which is again a direct result of content-addressable storage mechanism that is used by CernVM-FS. This is an essential security aspect since CernVM-FS uses (possibly untrusted) caches and HTTP connections to distribute the contents of a repository.</p> <p>For more technical details on how CernVM-FS implements these features, see here.</p>"}]}