{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Practices for CernVM-FS in HPC","text":"<p>Warning</p> <p>(Nov'23) This tutorial is under development, please come back later when the tutorial contents have been completed.</p> <p>An online version of this tutorial is planned for Mon 4 Dec 2023 (13:30-17:00 CET), register via https://event.ugent.be/registration/cvmfshpc202312.</p> <p>This is an introductory tutorial to CernVM-FS, the CernVM File System, with a focus on employing it in the context of High-Performance Computing (HPC).</p> <p>In this tutorial you will learn what CernVM-FS is, how to get access to existing CernVM-FS repositories, how to configure CernVM-FS, and how to use CernVM-FS repositories on HPC infrastructure.</p> <p>Ready to go? Click here to start the tutorial!</p>"},{"location":"#intended-audience","title":"Intended audience","text":"<p>This tutorial is intended for people with a background in HPC (system administrators, support team members, end users, etc.) and who are new to CernVM-FS; no specific prior knowledge or experience with it is required.</p> <p>We expect it to be most valuable to people who are interested in using or providing access to one or more existing CernVM-FS repositories on HPC infrastructure.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>(more info soon)</p>"},{"location":"#practical-information","title":"Practical information","text":"<p>A first virtual edition of this tutorial is planned for Monday 4 December 2023 (13:30-17:00 CET).</p> <p>Attendance is free, but registration is required: https://event.ugent.be/registration/cvmfshpc202312.</p> <p>(more practical info soon)</p>"},{"location":"#slides","title":"Slides","text":"<p>(coming soon)</p>"},{"location":"#multixscale","title":"MultiXscale","text":"<p>This tutorial is being developed and organised in the context of the MultiXscale EuroHPC Centre-of-Excellence.</p> <p>Funded by the European Union. This work has received funding from the European High Performance Computing Joint Undertaking (JU) and countries participating in the project under grant agreement No 101093169.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Jakob Blomer (CERN, Switzerland)</li> <li>Bob Dr\u00f6ge (University of Groningen, The Netherlands)</li> <li>Kenneth Hoste (Ghent University, Belgium)</li> <li>Alan O'Cais (University of Barcelona, Spain; CECAM)</li> <li>Lara Peeters (Ghent University, Belgium)</li> <li>Laura Promberger (CERN, Switzerland)</li> <li>Thomas R\u00f6blitz (University of Bergen, Norway)</li> <li>Caspar van Leeuwen (SURF, The Netherlands)</li> <li>Valentin V\u00f6lkl (CERN, Switzerland)</li> </ul>"},{"location":"#additional-resources","title":"Additional resources","text":"<ul> <li>CernVM-FS website</li> <li>CernVM-FS documentation</li> <li>CernVM-FS @ GitHub</li> <li>Introduction to CernVM-FS by Jakob Blomer (CERN) (2021)</li> <li>Introductory tutorial on CernVM-FS (2021)</li> </ul>"},{"location":"configuration_hpc/","title":"Configuring CernVM-FS on HPC infrastructure","text":"<p>In the previous section we have outlined how to set up a robust CernVM-FS infrastructure, by having a private Stratum 1 replica server and/or dedicated Squid proxy servers. While this approach will work for many HPC systems, some may have slightly more esoteric setups that require specific solutions, which we will discuss in this section.</p>"},{"location":"configuration_hpc/#diskless-worker-nodes","title":"Diskless worker nodes","text":"<p>Some HPC systems may have worker nodes without any type of local disk, which is problematic for CernVM-FS since it uses a local cache on the worker nodes. Without this local cache, CernVM-FS can not store the repository content that is being accessed by users.</p> <p>A couple of workarounds are possible in this case:</p> <ul> <li>In-memory client cache</li> <li>Loopback filesystem on a shared filesystem</li> <li>Alien cache</li> </ul>"},{"location":"configuration_hpc/#in-memory-client-cache","title":"In-memory client cache","text":"<p>An easy way to set up a client cache on diskless systems is to use a RAM disk like <code>/dev/shm</code>.</p> <p>It suffices to use a path like <code>/dev/shm/cvmfs-cache</code> (or equivalent) as the value for the <code>CVMFS_CACHE_BASE</code> configuration setting in <code>/etc/cvmfs/default.local</code>, along with setting <code>CVMFS_QUOTA_LIMIT</code> to the amount of memory that you would like to dedicate to the CernVM-FS client cache.</p> <p>For example:</p> <pre><code># use max. 4GB of memory for CernVM-FS client cache\nCVMFS_CACHE_BASE=/dev/shm/cvmfs-cache\nCVMFS_QUOTA_LIMIT=4000\n</code></pre> <p>Do not forget to apply the changes made by running:</p> <pre><code>sudo cvmfs_config reload\n</code></pre> <p>An obvious significant drawback of this is that less memory will be available to workloads running on the worker nodes, but it may be worth considering especially if enough memory is available in total.</p> <p>For general information on CernVM-FS cache settings, see the CernVM-FS documentation.</p>"},{"location":"configuration_hpc/#loopback-filesystem","title":"Loopback on shared filesystem","text":"<p>The generally recommended solution for diskless worker nodes is to use a loopback filesystem for the CernVM-FS client cache, which can be stored on the cluster's shared filesystem of the HPC cluster. Every worker node will need its own file in this case.</p> <p>This ensures that the parallelism of the shared file system can be exploited, while metadata accesses are performed within the loopback filesystems, and hence not overloading the shared filesystem's metadata server(s).</p> <p>The loopback filesystem files can be created using <code>dd</code> or <code>mkfs</code>. They should be formatted as an <code>ext3</code>, <code>ext4</code>, or <code>xfs</code> file system, and should be 15% larger than the cache size configured on the nodes (with <code>CVMFS_QUOTA_LIMIT</code>).</p> <p>On the worker nodes the loopback filesystem can be mounted from the shared file system, and they should be made available at the location specified in the <code>CVMFS_CACHE_BASE</code> configuration setting (or <code>/var/lib/cvmfs</code>, by default).</p>"},{"location":"configuration_hpc/#alien-cache-diskless","title":"Alien cache","text":"<p>An alien cache is a cache that is outside of the (full) control of CernVM-FS.</p> <p>In this scenario you store the cache on a shared filesystem, and have the CernVM-FS processes on all worker nodes use and fill it simultaneously. These processes can pull in the required files that are being accessed by users/jobs, or you can even manually preload the cache.</p> <p>Using the alien cache still requires a very small local cache on the worker nodes for storing some control files. Given its size, you can store this local cache on a shared filesystem, or in memory.</p> <p>Compared to using a loopback filesystem described in the previous subsection, the drawback of storing the alien cache on your shared filesystem is that all metadata operations are now performed on the shared filesystem. Typically, this will result in a large number of metadata operations, and on many shared filesystems will be a significant bottleneck.</p> <p>For more information about an alien cache and configuring it, see the Alien Cache section in the CernVM-FS documentation.</p>"},{"location":"configuration_hpc/#offline-worker-nodes","title":"Offline worker nodes","text":"<p>Another typical scenario for HPC systems is that worker nodes do not have (direct) access to the internet.</p> <p>In the context of CernVM-FS, this means that the clients running on the worker nodes are not be able to pull in files from (external) Stratum 1 replica servers.</p> <p>For this scenario, several solutions are available as well:</p> <ul> <li>Squid proxy in local network</li> <li>Private Stratum 1 replica server</li> <li>Alien cache</li> </ul>"},{"location":"configuration_hpc/#squid-local","title":"Squid proxy in local network","text":"<p>Setting up a Squid proxy server in the internal network of the cluster, which is highly recommended regardless of whether the worker nodes are offline or not, will circumvent this issue since the worker nodes can be configured to only connect to Stratum 1 servers via the Squid proxy.</p> <p>This means that only the Squid proxy server requires internet access in order to fetch files from the Stratum 1 servers, while the clients will fetch the files from the proxy using the internal network.</p>"},{"location":"configuration_hpc/#private-stratum1","title":"Private Stratum 1 replica server","text":"<p>Similar to having a Squid proxy in the internal network of the cluster, one could also opt for setting up a private Stratum 1 replica server that is accessible by the worker nodes, or even do both.</p> <p>Again, only the private Stratum 1 server needs to connect to the internet, as it will need to regularly synchronize with a synchronisation server.</p>"},{"location":"configuration_hpc/#alien-cache-offline","title":"Alien cache","text":"<p>As a last resort, you can consider use an alien cache that is being prepopulated on a dedicated system outside of the HPC, which does have internet access.</p> <p>This alien cache can then be made available on the worker nodes, for instance by having it stored on the shared filesystem of the cluster.</p> <p>This is (again) not recommended however, for the same reason as before: this kind of setup will put significant load on the metadata server(s) of the shared filesystem.</p>"},{"location":"configuration_hpc/#worker-nodes-without-cernvm-fs","title":"Worker nodes without CernVM-FS","text":"<p>The last scenario that we cover here is for HPC systems that do not have the CernVM-FS client component installed on the worker nodes, for example because the system administrators are not willing to install, configure, and maintain a CernVM-FS installation.</p> <p>Though less ideal than a native installaton of CernVM-FS, solutions to make CernVM-FS repositories accessible even in this case exist:</p> <ul> <li>Syncing a to another filesystem</li> <li>Alternative access mechanisms</li> </ul>"},{"location":"configuration_hpc/#sync-other-filesystem","title":"Syncing a to another filesystem","text":"<p>A seemingly straightforward solution may be to synchronize (a part of) the contents of a CernVM-FS repository to another filesystem, and make that available on worker nodes.</p> <p>CernVM-FS provides the <code>cvmfs_shrinkwrap</code> utility exactly for this purpose.</p> <p>However, though the solution may sound easy, it has some severe disadvantages: <code>cvmfs_shrinkwrap</code> utility puts a heavy load on the server that is being used to pull in the contents, as it has to fetch all the contents (which may be a large amount of data) in one large bulk operation.</p> <p>In addition, the repository contents will have to be synchronized in some way, which involves rerunning this process regularly.</p> <p>Finally, this approach somewhat defeats the purpose of CernVM-FS, as you will be replacing a filesystem that is optimized for distributing software by one that most likely is not.</p>"},{"location":"configuration_hpc/#alternatives","title":"Alternative access mechanisms","text":"<p>Alternative mechanisms to access CernVM-FS repositories exist that do not require system administrator privileges, so they can be leveraged by end users of HPC infrastructure.</p> <p>Examples include using a container runtime like Apptainer, or using <code>cvmfsexec</code>.</p> <p>For more details on these alternatives, see Alternative ways to access CernVM-FS repositories.</p> <p>Parrot connector to CernVM-FS</p> <p>While Parrot is still mentioned in the CernVM-FS documentation (see here), it is no longer recommended to use it, since better alternatives (like <code>cvmfsexec</code>) are available now.</p>"},{"location":"containers/","title":"Containers and CernVM-FS","text":"<p>CernVM-FS can also be used to distribute container images, providing many of the same benefits that come with any CernVM-FS installation. Especially the on-demand download of accessed files means that containers start nearly instantly, and are more efficient for large images when only a fraction of the files are read, which is typically the case.</p> <p>Any CernVM-FS repository can be used to distribute container images (although often, dedicated repositories are used, like <code>/cvmfs/unpacked.cern.ch</code>).</p> <p>In order to provide de-duplication and on-demand download, images must be stored unpacked. This requires some dedicated tools, provided by CernVM-FS itself - see the section \"Ingesting container images\" below.</p>"},{"location":"containers/#accessing-via-apptainer","title":"Accessing via Apptainer","text":"<p>Apptainer is the recommended way to run containers from CernVM-FS, as it can start a container directly from an unpacked root file system, which is ideal for CernVM-FS.</p> <p>Docker can be used as well but the setup is more complicated, requiring the CernVM-FS graphdriver plugin.</p> <p>For example, to run the <code>tensorflow/tensorflow:2.15.0-jupyter</code> image from Docker Hub that has been unpacked in <code>/cvmfs/unpacked.cern.ch</code>, use the following commands:</p> <pre><code>container=\"registry.hub.docker.com/tensorflow/tensorflow:2.15.0-jupyter\"\npython_code=\"import tensorflow as tf; print(tf.__version__)\"\napptainer run /cvmfs/unpacked.cern.ch/${container} python -c \"${python_code}\"\n</code></pre> <p>This directory just contains the root file system of the image:</p> <pre><code>ls /cvmfs/unpacked.cern.ch/registry.hub.docker.com/tensorflow/tensorflow:2.15.0-jupyter\n</code></pre>"},{"location":"containers/#ingesting","title":"Ingesting container images","text":"<p>CernVM-FS provides a suite of container unpacking tools called <code>cvmfs_ducc</code> (provided by the <code>cvmfs-ducc</code> package). This can be used to unpack and ingest container images by simply running</p> <p><pre><code>cvmfs_ducc convert recipe.yaml \n</code></pre> where <code>recipe.yaml</code> is a 'wishlist' of container images available in external registries that should be made available:</p> <pre><code>version: 1\nuser: cvmfsunpacker\ncvmfs_repo: 'unpacked.repo.tld'\ninput:\n    - 'https://registry.hub.docker.com/tensorflow/tensorflow:2.15.0-jupyter'\n    ...\n</code></pre> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"containers/#using-cvmfs-inside-containers","title":"Using <code>/cvmfs</code> inside containers","text":"<p>The easiest way to access CernVM-FS repositories from a container is to set it up on the host and bind-mount it inside the container:</p> <pre><code>docker run -it --volume /cvmfs:/cvmfs:shared ubuntu ls -lna /cvmfs/atlas.cern.ch\n</code></pre> <p>For Apptainer, the same can be done by setting the <code>$SINGULARITY_BIND</code> (or <code>$APPTAINER_BIND</code>) environment variable: </p> <pre><code>export SINGULARITY_BIND=\"/cvmfs\"\n</code></pre>"},{"location":"getting-started/","title":"Getting started with CernVM-FS (from scratch)","text":""},{"location":"getting-started/#setting-up-the-stratum-0-server","title":"Setting up the Stratum-0 server","text":""},{"location":"getting-started/#creating-a-cernvm-fs-repository","title":"Creating a CernVM-FS repository","text":""},{"location":"getting-started/#setting-up-a-stratum-1-replica-server","title":"Setting up a Stratum-1 replica server","text":""},{"location":"performance/","title":"Performance aspects of CernVM-FS","text":""},{"location":"performance/#startup-performance","title":"Startup performance","text":""},{"location":"performance/#os-jitter-by-cernvm-fs-daemon","title":"OS jitter by CernVM-FS daemon","text":""},{"location":"performance/#using-a-cdn","title":"Using a CDN","text":""},{"location":"troubleshooting/","title":"Troubleshooting and debugging CernVM-FS","text":""},{"location":"troubleshooting/#logs","title":"Logs","text":""},{"location":"troubleshooting/#stats","title":"Stats","text":""},{"location":"troubleshooting/#common-problems","title":"Common problems","text":""},{"location":"troubleshooting/#monitoring","title":"Monitoring","text":""},{"location":"troubleshooting/#mounting-in-debug-mode","title":"Mounting in debug mode","text":""},{"location":"access/","title":"Accessing CernVM-FS repositories","text":"<ul> <li>Setting up a CernVM-FS client system</li> <li>Setting up a proxy server</li> <li>Setting up a Stratum 1 replica server</li> <li>Alternative ways to access CernVM-FS repositories</li> </ul>"},{"location":"access/alternatives/","title":"Alternative ways to access CernVM-FS repositories","text":"<p>While a native installation of CernVM-FS on the client system, along with a proxy server and/or Stratum 1 replica server for large-scale production setups, is recommended, there are other alternatives available for getting access to CernVM-FS repositories.</p> <p>We briefly cover some of these here, mostly to clarify that there are alternatives available, including some that do not require system administrator permissions.</p>"},{"location":"access/alternatives/#cvmfsexec","title":"<code>cvmfsexec</code>","text":"<p>Using <code>cvmfsexec</code>, mounting of CernVM-FS repositories as an unprivileged user is possible, without having CernVM-FS installed system-wide.</p> <p><code>cvmfsexec</code> supports multiple ways of doing this depending on the OS version and system configuration, more specifically whether or not particular features are enabled, like:</p> <ul> <li>FUSE mounting with <code>fusermount</code>;</li> <li>unprivileged user namespaces;</li> <li>unprivileged namespace fuse mounts;</li> <li>a <code>setuid</code> installation of Singularity 3.4+ (via <code>singcvmfs</code> which uses the <code>--fusemount</code> feature),   or an unprivileged installation of Singularity 3.6+;</li> </ul> <p>Start by cloning the <code>cvmfsexec</code> repository from GitHub, and change to the <code>cvmfsexec</code> directory:</p> <pre><code>git clone https://github.com/cvmfs/cvmfsexec.git\ncd cvmfsexec\n</code></pre> <p>Before using <code>cvmfsexec</code>, you first need to make a <code>dist</code> directory that includes CernVM-FS, configuration files, and scripts. For this, you can run the <code>makedist</code> script that comes with <code>cvmfsexec</code>:</p> <pre><code>./makedist default\n</code></pre> <p>With the <code>dist</code> directory in place, you can use <code>cvmfsexec</code> to run commands in an environment where a CernVM-FS repository is mounted.</p> <p>For example, we can run a script named <code>test_eessi.sh</code> that contains:</p> <pre><code>#!/bin/bash\n\nsource /cvmfs/software.eessi.io/versions/2023.06/init/bash\n\nmodule load TensorFlow/2.13.0-foss-2023a\n\npython -V\npython3 -c 'import tensorflow as tf; print(tf.__version__)'\n</code></pre> <p>which gives: <pre><code>$ ./cvmfsexec software.eessi.io -- ./test_eessi.sh\n\nCernVM-FS: loading Fuse module... done\nCernVM-FS: mounted cvmfs on /home/rocky/cvmfsexec/dist/cvmfs/cvmfs-config.cern.ch\nCernVM-FS: loading Fuse module... done\nCernVM-FS: mounted cvmfs on /home/rocky/cvmfsexec/dist/cvmfs/software.eessi.io\n\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n\nPython 3.11.3\n2.13.0\n</code></pre></p> <p>By default, the CernVM-FS client cache directory will be located in <code>dist/var/lib/cvmfs</code>.</p> <p>For more information on <code>cvmfsexec</code>, see https://github.com/cvmfs/cvmfsexec.</p>"},{"location":"access/alternatives/#apptainer-with-fusemount","title":"Apptainer with <code>--fusemount</code>","text":"<p>If Apptainer is available, you can get access to a CernVM-FS repository by using a container image that includes the CernVM-FS client component (see for example the Docker recipe for the client container used in EESSI, which is available here).</p> <p>Using the <code>--fusemount</code> option you can specify that a CernVM-FS repository should be mounted when starting the container. For example for EESSI, you should use:</p> <pre><code>apptainer ... --fusemount \"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\" ...\n</code></pre> <p>There are a couple of caveats here:</p> <ul> <li> <p>If the configuration for the CernVM-FS repository is provided via the <code>cvmfs-config</code> repository,   you need to instruct Apptainer to also mount that, by using the <code>--fusemount</code> option twice: once for   the <code>cvmfs-config</code> repository, and once for the target repository itself:   <pre><code>FUSEMOUNT_CVMFS_CONFIG=\"container:cvmfs2 cvmfs-config.cern.ch /cvmfs/cvmfs-config.cern.ch\"\nFUSEMOUNT_EESSI=\"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\"\napptainer ... --fusemount \"${FUSEMOUNT_CVMFS_CONFIG}\" --fusemount \"${FUSEMOUNT_EESSI}\" ...\n</code></pre></p> </li> <li> <p>Next to mounting CernVM-FS repositories, you also need to bind mount local writable directories   to <code>/var/run/cvmfs</code>, since CernVM-FS needs write access in those locations (for the CernVM-FS client cache):   <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs}\nexport APPTAINER_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\napptainer ... --fusemount ...\n</code></pre></p> </li> </ul> <p>To try this, you can use the EESSI client container that is available in Docker Hub, to start an interactive shell in which EESSI is available, as follows:</p> <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs}\nexport APPTAINER_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\nFUSEMOUNT_CVMFS_CONFIG=\"container:cvmfs2 cvmfs-config.cern.ch /cvmfs/cvmfs-config.cern.ch\"\nFUSEMOUNT_EESSI=\"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\"\napptainer shell --fusemount \"${FUSEMOUNT_CVMFS_CONFIG}\" --fusemount \"${FUSEMOUNT_EESSI}\" docker://ghcr.io/eessi/client-pilot:centos7\n</code></pre>"},{"location":"access/alternatives/#alien-cache","title":"Alien cache","text":"<p>An alien cache can be used, optionally in combination with preloading, as another alternative, typically in combination with using a container image or unprivileged user namespaces.</p> <p>For more information, see the Alien cache subsection in the next part of the tutorial.</p> <p>(next: Configuration on HPC systems)</p>"},{"location":"access/client/","title":"CernVM-FS client system","text":"<p>The recommended way to gain access to CernVM-FS repositories is to set up a system-wide native installation of CernVM-FS on the client system(s), which comes down to:</p> <ul> <li>Installing the client component of CernVM-FS;</li> <li>Creating a minimal client configuration file (<code>/etc/cvmfs/default.local</code>);</li> <li>Completing the client setup by:<ul> <li>Creating a<code>cvmfs</code> user account and group;</li> <li>Creating the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories;</li> <li>Configuring <code>autofs</code> to enable auto-mounting of repositories (recommended).</li> </ul> </li> </ul> <p>For repositories that are not included in the default CernVM-FS configuration you also need to provide some additional information specific to those repositories in order to access them.</p> <p>This is not a production-ready setup (yet)!</p> <p>While these basic steps are enough to gain access to CernVM-FS repositories, this is not sufficient to obtain a production-ready setup.</p> <p>This is especially true on HPC infrastructure that typically consists of a large number of worker nodes on which software provided by one or more CernVM-FS repositories will be used.</p> <p>After covering the basic client setup in this section, we will outline how to make accessing of CernVM-FS repositories more reliable and performant, by also setting up a proxy server and CernVM-FS Stratum 1 replica server.</p>"},{"location":"access/client/#installing-cernvm-fs-client","title":"Installing CernVM-FS client","text":"<p>Start with installing the <code>cvmfs</code> package which provides the CernVM-FS client component:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code># install cvmfs-release package to add yum repository\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n# install CernVM-FS client package\nsudo yum install -y cvmfs\n</code></pre> <pre><code># install cvmfs-release package to add apt repository\nsudo apt install lsb-release\ncurl -OL https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nsudo apt update\n\n# install CernVM-FS client package\nsudo apt install -y cvmfs\n</code></pre> <p>If none of the available <code>cvmfs</code> packages are compatible with your system, you can also build CernVM-FS from source.</p>"},{"location":"access/client/#minimal_configuration","title":"Minimal client configuration","text":"<p>Next to installing the CernVM-FS client, you should also create a minimal configuration file for it.</p> <p>This is typically done in <code>/etc/cvmfs/default.local</code>, which should contain something like:</p> <pre><code>CVMFS_CLIENT_PROFILE=\"single\" # a single node setup, not a cluster\nCVMFS_QUOTA_LIMIT=10000\n</code></pre> <p>More information on the structure of <code>/etc/cvmfs</code> and supported configuration settings is available in the CernVM-FS documentation.</p> Client profile setting (click to expand) <p>With <code>CVMFS_CLIENT_PROFILE=\"single\"</code> we specify that this CernVM-FS client should:</p> <ul> <li>Use the proxy server specified via <code>CVMFS_HTTP_PROXY</code>, if that configuration setting is defined;</li> <li>Directly connect to a Stratum-1 replica server that provides the repository being used if no proxy server   is specified via <code>CVMFS_HTTP_PROXY</code>.</li> </ul> <p>As an alternative to defining <code>CVMFS_CLIENT_PROFILE</code>, you can also set <code>CVMFS_HTTP_PROXY</code> to <code>DIRECT</code> to specify that no proxy server should be used by CernVM-FS:</p> <pre><code>CVMFS_HTTP_PROXY=\"DIRECT\"\n</code></pre> <p>We will get back to <code>CVMFS_HTTP_PROXY</code> later when setting up a proxy server.</p> Maximum size of client cache (click to expand) <p>The <code>CVMFS_QUOTA_LIMIT</code> configuration setting specifies the maximum size of the CernVM-FS client cache (in MBs).</p> <p>In the example above, we specify that no more than ~10GB should be used for the client cache.</p> <p>When the specified quota limit is reached, CernVM-FS will automatically remove files from the cache according to the Least Recently Used (LRU) policy, until half of the maximum cache size has been freed.</p> <p>The location of the cache directory can be controlled by <code>CVMFS_CACHE_BASE</code> if needed (default: <code>/var/lib/cvmfs</code>), but must be a on a local file system of the client, not a network file system that can be modified by multiple hosts.</p> <p>Using a directory in a RAM disk (like <code>/dev/shm</code>) for the CernVM-FS client cache can be considered if enough memory is available in the client system, which would help reduce latency and start-up performance of software.</p> <p>For more information on cache-related configuration settings, see the CernVM-FS documentation.</p>"},{"location":"access/client/#completing-the-client-setup","title":"Completing the client setup","text":"<p>To complete the setup of the CernVM-FS client component, we need to make sure that a <code>cvmfs</code> service account and group are present on the system, and the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories exist with the correct ownership and permissions.</p> <p>This should be taken care of by the post-install script that is run when installing the <code>cvmfs</code> package, so you will only need to take action on these aspects if you were installing the CernVM-FS client from source.</p> <p>In addition, it is recommended to update the <code>autofs</code> configuration to enable auto-mounting of CernVM-FS repositories, and to make sure the <code>autofs</code> service is running.</p> <p>All these actions can be performed in one go by running the following command:</p> <pre><code>sudo cvmfs_config setup\n</code></pre> <p>Additional options can be passed to the <code>cvmfs_config setup</code> command to disable some of the actions, like <code>nouser</code> to not create the <code>cvmfs</code> user and group, or <code>noautofs</code> to not update the <code>autofs</code> configuration.</p>"},{"location":"access/client/#autofs","title":"Recommendations for <code>autofs</code>","text":"<p>It is recommended to configure <code>autofs</code> to never unmount repositories due to inactivity, since that can cause problems in specific situations.</p> <p>This can be done by setting additional options in <code>/etc/sysconfig/autofs</code> (on RHEL-based Linux distributions) or <code>/etc/default/autofs</code> (on Debian-based distributions):</p> <pre><code>OPTIONS=\"--timeout 0\"\n</code></pre> <p>The default <code>autofs</code> timeout is typically 5 minutes (300 seconds), which is usually specified in <code>/etc/autofs.conf</code>.</p>"},{"location":"access/client/#using-static-mounts","title":"Using static mounts","text":"<p>If you prefer not to use <code>autofs</code>, you will need to use static mounting, by either:</p> <ul> <li> <p>Manually mounting the CernVM-FS repositories you want to use, for example:   <pre><code>sudo mkdir -p /cvmfs/software.eessi.io\nsudo mount -t cvmfs software.eessi.io /cvmfs/software.eessi.io\n</code></pre></p> </li> <li> <p>Updating <code>/etc/fstab</code> to ensure that the CernVM-FS repositories are mounted at boot time.</p> </li> </ul> <p>Configuring <code>autofs</code> to never unmount due to inactivity is preferable to using static mounts.</p> <p>For more information on mounting repositories, see the CernVM-FS documentation.</p>"},{"location":"access/client/#checking-client-setup","title":"Checking client setup","text":"<p>To ensure that the setup of the CernVM-FS client component is valid, you can run:</p> <pre><code>sudo cvmfs_config chksetup\n</code></pre> <p>You should see <code>OK</code> as output of this command.</p>"},{"location":"access/client/#default-repositories","title":"Default repositories","text":"<p>The default configuration of CernVM-FS, provided by the <code>cvmfs-config-default</code> package, provides the public keys and configuration for a number of commonly used CernVM-FS repositories.</p> <p>One particular repository included in the default CernVM-FS configuration is <code>cvmfs-config.cern.ch</code>, which is a CernVM-FS config repository that provides public keys and configuration for additional flagship CernVM-FS repositories, like <code>software.eessi.io</code>:</p> <pre><code>$ ls /cvmfs/cvmfs-config.cern.ch/etc/cvmfs\ncommon.conf  config.d  default.conf  domain.d  keys\n\n$ find /cvmfs/cvmfs-config.cern.ch/etc/cvmfs -type f -name '*eessi*'\n/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/domain.d/eessi.io.conf\n/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/keys/eessi.io/eessi.io.pub\n</code></pre>"},{"location":"access/client/#inspecting-repository-configuration","title":"Inspecting repository configuration","text":"<p>To check whether a specific CernVM-FS repository is accessible, we can probe it:</p> <pre><code>$ cvmfs_config probe software.eessi.io\nProbing /cvmfs/software.eessi.io... OK\n</code></pre> <p>To view the configuration for a specific repository, use <code>cvmfs_config showconfig</code>: <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre></p> <p>To check the active configuration for a specific repository used by the running CernVM-FS instance, use <code>cvmfs_talk -i &lt;repo&gt; parameters</code> (which requires admin privileges):</p> <pre><code>sudo cvmfs_talk -i software.eessi.io parameters\n</code></pre> <p><code>cvmfs_talk</code> requires that the repository is currently mounted. If not, you will see an error like this:</p> <pre><code>$ sudo cvmfs_talk -i software.eessi.io parameters\nSeems like CernVM-FS is not running in /var/lib/cvmfs/shared (not found: /var/lib/cvmfs/shared/cvmfs_io.software.eessi.io)\n</code></pre>"},{"location":"access/client/#accessing-a-repository","title":"Accessing a repository","text":"<p>To access the contents of the repository, just use the corresponding subdirectory as if it were a local filesystem.</p> <p>While the contents of the files you are accessing are not actually available on the client system the first time they are being accessed, CernVM-FS will automatically downloaded them in the background, providing the illusion that the whole repository is already there.</p> <p>We like to refer to this as \"streaming\" of software installations, much like streaming music or video services.</p> <p>To start using EESSI just source the initialisation script included in the repository:</p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash\n</code></pre> <p>You may notice some \"lag\" when files are being accessed, or not, depending on the network latency.</p>"},{"location":"access/client/#additional-repositories","title":"Additional repositories","text":"<p>To access additional CernVM-FS repositories beyond those that are available by default, you will need to:</p> <ul> <li>Add the public keys for those repositories into a domain-specific subdirectory of <code>/etc/cvmfs/keys/</code>;</li> <li>Add the configuration for those repositories into <code>/etc/cvmfs/domain.d</code> (domain-specific) or <code>/etc/cvmfs/config.d</code> (repository-specific).</li> </ul> <p>Examples are available in the <code>etc/cvmfs</code> subdirectory of the config-repo GitHub repository.</p> <p>(next: Squid proxy server)</p>"},{"location":"access/proxy/","title":"Squid proxy server","text":"<p>As a first step towards a production-ready CernVM-FS setup we can install a Squid forward proxy server, which is strongly recommended in the context of HPC systems.</p> <p>The proxy server will (often dramatically) reduce the latency for client systems for accessing CernVM-FS repositories, and hence significantly improve start-up performance of software provided via CernVM-FS. In addition, it reduces the load on the Stratum 1 replica servers that support the repositories being used.</p> <p>This is particularly important when running large-scale MPI software, since the software binary and all the libraries it requires must be available on all worker nodes being employed before the software can start running.</p>"},{"location":"access/proxy/#general-recommendations","title":"General recommendations","text":"<p>The proxy server should have a 10Gbit link to the client systems, a sufficiently powerful CPU, a decent amount of memory for the kernel cache (tens of GBs), and fast local storage (SSD or NVMe).</p> <p>It is strongly recommended to have at least two proxy servers available, to have some redundancy available in case of unexpected problems, or during a maintenance window.</p> <p>As a rule of thumb, it is recommended to have (at least) one proxy server for every couple of hundred worker nodes (100-500).</p> <p>Note that the load on the proxy servers used by CernVM-FS is highly dependent on the workload mix on the client systems.</p>"},{"location":"access/proxy/#proxy-server-setup","title":"Proxy server setup","text":""},{"location":"access/proxy/#installation","title":"Installation","text":"<p>First, install the <code>squid</code> package using your OS package manager:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code>sudo yum install -y squid\n</code></pre> <pre><code>sudo apt install -y squid\n</code></pre>"},{"location":"access/proxy/#configuration","title":"Configuration","text":"<p>Create a configuration file for the Squid proxy in <code>/etc/squid/squid.conf</code>.</p> <p>You can use the following template for this:</p> <pre><code># List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy\nacl local_nodes src YOUR_CLIENT_IPS\n\n# Destination domains that are allowed\nacl stratum_ones dstdomain .YOURDOMAIN.ORG\n\n# Squid port\nhttp_port 3128\n\n# Deny access to anything which is not part of our stratum_ones ACL.\nhttp_access deny !stratum_ones\n\n# Only allow access from our local machines\nhttp_access allow local_nodes\nhttp_access allow localhost\n\n# Finally, deny all other access to this proxy\nhttp_access deny all\n\nminimum_expiry_time 0\nmaximum_object_size 1024 MB\n\n# proxy memory cache of 1GB\ncache_mem 1024 MB\nmaximum_object_size_in_memory 128 KB\n# 50 GB disk cache\ncache_dir ufs /var/spool/squid 50000 16 256\n</code></pre> <p>In this template, you must change two things in the Access Control List (ACL) settings:</p> <p>1) Specify which client systems can access your proxy by replacing \"<code>YOUR_CLIENT_IPS</code>\" with the corresponding IP range, using CIDR notation;</p> <p>2) Make sure that the proxy server allows access to the Stratum 1 replica servers that are relevant for the CernVM-FS repositories    you are using, by replacing \"<code>.YOURDOMAIN.ORG</code>\" with domain name for the Stratum 1 replica servers    (see also the Squid ACL documentation).</p> <p>For example, to allow connecting to the EESSI Stratum 1 replica servers, use:</p> <pre><code>acl stratum_ones dstdomain .eessi.science\n</code></pre> <p>Note that this configuration assumes that port 3128 is accessible on the proxy server.</p> <p>To check your Squid configuration, use:</p> <pre><code>sudo squid -k parse\n</code></pre> <p>If no warnings or errors are printed by this command, you should be all set (assuming that the ACLs are set correctly).</p> <p>For more information on configuring a Squid proxy, see the CernVM-FS documentation,</p>"},{"location":"access/proxy/#starting-the-service","title":"Starting the service","text":"<p>To start the Squid and enable it on booting the proxy server, run:</p> <pre><code>sudo systemctl start squid\nsudo systemctl enable squid\n</code></pre> <p>To check the status of the Squid, you can use:</p> <pre><code>sudo systemctl status squid\n</code></pre>"},{"location":"access/proxy/#client-system-configuration","title":"Client system configuration","text":"<p>To make a CernVM-FS client system use the proxy server, the <code>/etc/cvmfs/default.local</code> configuration file on the client system should be updated to include:</p> <pre><code># replace PROXY_IP with the IP address of the proxy server\nCVMFS_HTTP_PROXY=\"http://PROXY_IP:3128\"\n</code></pre> <p>To apply the change we need to reload the CernVM-FS configuration on the client system:</p> <pre><code>sudo cvmfs_config reload\n</code></pre> <p>You can test the new configuration and verify whether the proxy is indeed being used by the client system via <code>cvmfs_config stat</code>:</p> <pre><code>ls /cvmfs/software.eessi.io\ncvmfs_config stat -v software.eessi.io\n</code></pre> <p>We first inspect the contents of the repository using <code>ls</code> to make sure that the repository is mounted, which is assumed by <code>cvmfs_config stat</code>.</p> <p>The output of the <code>stat</code> command should look something like this:</p> <p>Note the <code>Connection</code> line, which clearly shows that the proxy server is used (and is working):</p> <pre><code>Connection: .../software.eessi.io through proxy http://PROXY_IP:3128 (online)\n</code></pre> <p>To check whether the proxy is working as intended you can use <code>curl</code> to try to access the <code>.cvmfspublished</code> file in the root of the repository on a Stratum 1, for example:</p> <pre><code># replace PROXY_IP with the IP address of the proxy server\nhttp_proxy=http://PROXY_IP:3128 curl --head http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> <p>The first output line of this command should be:</p> <pre><code>HTTP/1.1 200 OK\n</code></pre>"},{"location":"access/proxy/#cleanup_proxy","title":"Cleanup to prepare for Stratum 1","text":"<p>To prepare for the next tutorial section on setting up a private Stratum 1 replica server, comment out the <code>CVMFS_HTTP_PROXY</code> line in <code>/etc/cvmfs/default.local</code> by prefixing it with a hash (<code>#</code>):</p> <pre><code>#CVMFS_HTTP_PROXY=\"http://PROXY_IP:3128\"\n</code></pre> <p>The full contents of <code>/etc/cvmfs/default.local</code> on the client system should again be as shown below, like it was initially created when configuring the client:</p> <pre><code>CVMFS_CLIENT_PROFILE=\"single\"\nCVMFS_QUOTA_LIMIT=10000\n</code></pre> <p>Do not forget to also reload the CernVM-FS client configuration: <pre><code>sudo cvmfs_config reload\n</code></pre></p> <p>We will later reconfigure the proxy server so it can be used together with our private Stratum 1.</p> <p>(next: Setting up a Stratum 1 replica server)</p>"},{"location":"access/stratum1/","title":"Private Stratum 1 replica server","text":"<p>In this section of the tutorial, we will set up a Stratum 1 replica server, which is the next step towards a production-ready CernVM-FS setup.</p>"},{"location":"access/stratum1/#cernvm-fs-as-cdn","title":"CernVM-FS as CDN","text":"<p>The content of CernVM-FS repositories is served by a set of Stratum 1 replica servers (sometimes also called mirror servers), which together with the central Stratum 0 server and the proxy servers can be seen as a content delivery network (CDN).</p> <p>A Stratum 1 replica server is a standard web server that uses the CernVM-FS server tools to provide a full mirror of one or more CernVM-FS repositories, which are served and managed through a central Stratum 0 server.</p> <p>The figure below shows the CernVM-FS network for repositories in the <code>cern.ch</code> domain, including the Stratum 1 replica servers which are spread all across the world, and a distributed hierarchy of proxy servers which fetch content from the closest Stratum 1.</p> <p> </p>"},{"location":"access/stratum1/#motivation","title":"Motivation","text":"<p>Next to the public Stratum 1 servers that are operated by the maintainers of a CernVM-FS repository, you can also set up your own \"private\" Stratum 1 replica server in your local network.</p> <p>In the context of using CernVM-FS on HPC infrastructure this brings the following benefits:</p> <ul> <li>To improve the overall reliability of the setup, for example in case of (temporary) loss of connectivity to the public Stratum 1 replica servers;</li> <li>To reduce the load on the public Stratum 1 servers;</li> <li>To mitigate the impact of poor network bandwidth to the closest public Stratum 1 server;</li> <li>To improve the latency and hence start-up time of software in situations where the cache of the proxy servers has insufficient capacity;</li> </ul>"},{"location":"access/stratum1/#recommendations","title":"Recommendations","text":"<p>When setting up a Stratum 1 replica server, you should take the following recommendations into account:</p> <ul> <li>A RAID-protected low latency storage setup (like SSD   or NVMe) should be used,   because the CernVM-FS server component will run lots of <code>stat</code> system calls against it.</li> <li>An <code>ext3</code> or <code>ext4</code> file system   is preferred (rather than XFS).</li> <li>A standard Apache web server should be installed, which should be close to the   low latency storage. Directory listing is not required.</li> <li>HTTP connections to port <code>80</code> must be possible.</li> </ul> Recommendations on monitoring (click to expand) <p>It is strongly recommended to actively monitor a Stratum 1 replica server, in particular:</p> <ul> <li>CPU usage;</li> <li>disk usage;</li> <li>I/O load;</li> <li>network bandwidth and latency;</li> <li>log messages produced in syslog;</li> </ul> <p>The <code>cvmfs-servermon</code> package can be used to watch for problems in every repository\u2019s <code>.cvmfs_status.json</code> status file.</p> <p>See also the CernVM-FS documentation.</p> Recommendations for a high-availability setup (click to expand) <p>To create a high-availability setup, it is not recommended to use two or more separate Stratum 1 replica servers in a single round-robin service.</p> <p>Since they will be updated at different rates, that would cause errors when a client sees an updated catalog from one Stratum 1, but then tries to read corresponding data files from another that does not yet have the files.</p> <p>Instead, different Stratum 1 replica servers should either be separately configured on the clients, or a pair can be configured  as a high availability active/standby pair using the <code>cvmfs-hastratum1</code> package. </p> <p>An active/standby pair can also be managed by switching a DNS name between two different servers.</p> Recommendations for a public Stratum 1 replica server (click to expand) <p>For a public Stratum 1 replica server, it is recommended to install a Squid frontend in front of the Stratum 1, which should be configured as a reverse proxy, and installed on the same system as the web server, to reduce the number of points of failure. The optimized <code>frontier-squid</code> distribution is recommended. For more information, see the CernVM-FS configuration.</p> <p>Alternatively, separate Squid proxy server machines can be configured in a round-robin DNS configuration and each forward to the Apache server. Note however that if any of them are down the entire service will be considered down by CernVM-FS clients. The impact of this can be mitigated through front end hardware load balancer that quickly takes a system that is down out of service.</p> Recommendations on garbage collection (click to expand) <p>If any CernVM-FS repositories being replicated have garbage collection enabled, the Stratum 1 also needs to run garbage collection in order to prevent the disk space usage from growing rapidly.</p> <p>See the CernVM-FS documentation for more details.</p> Using S3-compatible storage (Amazon S3, Azure Blob, Ceph) <p>CernVM-FS can store data directly to S3-compatible storage systems, such as Amazon S3, Azure Blob, or Ceph.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"access/stratum1/#setup-procedure","title":"Setup procedure","text":"<p>To set up a Stratum 1 replica server and configure it to replicate a particular CernVM-FS repository, you should:</p> <ul> <li>Install the <code>cvmfs-server</code> package;</li> <li>Add the public key of the CernVM-FS repository you want to replicate to <code>/etc/cvmfs/keys/</code>;</li> <li>Create the repository replica;</li> <li>Run the initial synchronisation;</li> <li>Configure <code>cron</code> to perform periodic synchronisation;</li> </ul> <p>In the sections below, we will set up a Stratum 1 replica server for the EESSI CernVM-FS repository <code>software.eessi.io</code>.</p>"},{"location":"access/stratum1/#installing-cernvm-fs-server","title":"Installing CernVM-FS server","text":"<p>Start with installing the <code>cvmfs-server</code> package which provides the CernVM-FS server tools.</p> <p>Although we won't actually use the functionality that requires it, we also need to install a package that provides the <code>mod_wsgi</code> Apache adapter module.</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code># install cvmfs-release package to add yum repository\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n# install CernVM-FS server package\nsudo yum install -y cvmfs-server\n\n# install mod_wsgi Apache adapter module\n# (on versions older than equivalent to RHEL8, install mod_wsgi instead)\nsudo yum install -y python3-mod_wsgi\n</code></pre> <pre><code># install cvmfs-release package to add apt repository\nsudo apt install lsb-release\ncurl -OL https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nsudo apt update\n\n# install CernVM-FS server package + the required mod-wsgi\nsudo apt install -y cvmfs-server\n\n# install mod_wsgi Apache adapter module\n# (on Ubuntu versions older than 22.04, install libapache2-mod-wsgi instead)\nsudo apt install -y libapache2-mod-wsgi-py3\n</code></pre>"},{"location":"access/stratum1/#adding-repository-public-key","title":"Adding repository public key","text":"<p>Add the public key for the repositories in the <code>eessi.io</code> domain to <code>/etc/cvmfs/keys</code>:</p> <pre><code>sudo mkdir -p /etc/cvmfs/keys/eessi.io/\nsudo cp eessi.io.pub /etc/cvmfs/keys/eessi.io/\n</code></pre> <p>You can get the contents for <code>eessi.io.pub</code> from the default CernVM-FS config repository on a CernVM-FS client system at:</p> <pre><code>/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/keys/eessi.io/eessi.io.pub\n</code></pre>"},{"location":"access/stratum1/#creating-repository-replica","title":"Creating repository replica","text":"<p>To create the repository replica, we need to use run the <code>cvmfs_server add-replica</code> command.</p>"},{"location":"access/stratum1/#synchronisation-server","title":"Synchronisation server","text":"<p>We will need to specify the server that should be used for synchronising the repository contents. This can either be the Stratum 0 server, or a public Stratum 1 replica server that was set up to be used for repository synchronisation (by having a <code>.cvmfs_master_replica</code> file in the HTTP root directory).</p> <p>For EESSI we should use <code>aws-eu-west-s1-sync.eessi.science</code> as synchronisation server.</p>"},{"location":"access/stratum1/#disabling-the-geoapi","title":"Disabling the GeoAPI","text":"<p>Before creating the replica, we first need to disable the Geo API service in the CernVM-FS server configuration, to avoid getting this error when creating the replica:</p> <pre><code>Installing GeoIP Database... CVMFS_GEO_LICENSE_KEY not set\nfail\n</code></pre> <p>The Geo API service enables client systems to automatically sort Stratum 1 replica servers geographically, so the CernVM-FS client component can prioritize connecting to the closest one.</p> <p>This is really only relevant for public Stratum 1 replica servers, not a private Stratum 1 replica server that is only accessible from within the local network, like the one we are setting up here.</p> <p>To disable the Geo API service, set <code>CVMFS_GEO_DB_FILE</code> to <code>NONE</code> in <code>/etc/cvmfs/server.local</code>:</p> <pre><code>echo 'CVMFS_GEO_DB_FILE=NONE' | sudo tee -a /etc/cvmfs/server.local\n</code></pre>"},{"location":"access/stratum1/#creating-replica","title":"Creating replica","text":"<p>To actually create the replica, run the <code>cvmfs_server add-replica</code> command as follows, specifying that the current user account should be the repository owner via <code>-o $USER</code>:</p> <pre><code>sync_server='aws-eu-west-s1-sync.eessi.science'\nrepo='software.eessi.io'\nkey_dir='/etc/cvmfs/keys/eessi.io'\nsudo cvmfs_server add-replica -o $USER http://${sync_server}/cvmfs/${repo} ${key_dir}\n</code></pre> Starting Apache (click to expand) <p>If creating the replica fails with:</p> <pre><code>Apache must be installed and running\n</code></pre> <p>try starting the <code>httpd</code> service first:</p> <pre><code>sudo systemctl start httpd.service\nsudo systemctl enable httpd.service\n</code></pre>"},{"location":"access/stratum1/#initial-synchronisation","title":"Initial synchronisation","text":"<p>After creating the replica, we should trigger the initial synchronisation of the repository replica, using the <code>cvmfs_server snapshot</code> command:</p> <pre><code>cvmfs_server snapshot software.eessi.io\n</code></pre> <p>Time for a coffee...</p> <p>Since this will download the full repository contents from the synchronisation server that was specified when creating the repository replica, the initial synchronisation may take a while.</p> <p>The time required for the initial synchronisation is heavily dependent on the size of the repository, and the available network latency to the synchronisation server.</p>"},{"location":"access/stratum1/#periodic-synchronisation","title":"Periodic synchronisation","text":"<p>To ensure that updates to the contents of the CernVM-FS repository are synchronised automatically to the Stratum-1 replica server, we should set up a cron job to do periodic synchronisation by running <code>cvmfs_server snapshot -a</code>.</p>"},{"location":"access/stratum1/#log-rotation","title":"Log rotation","text":"<p>Before setting up a cron job, we first need to configure log rotation, or running <code>snapshot -a</code> will fail with: <pre><code>/etc/logrotate.d/cvmfs does not exist!\n</code></pre></p> <p>Create <code>/etc/logrotate.d/cvmfs</code> with the following contents:</p> <pre><code>/var/log/cvmfs/*.log {\n    weekly\n    missingok\n    notifempty\n}\n</code></pre>"},{"location":"access/stratum1/#cron-job","title":"Cron job","text":"<p>To synchronize all active replica repositories every 5 minutes, we can create a cron job <code>/etc/cron.d/cvmfs_stratum1_snapshot</code> that runs <code>cvmfs_server snapshot -a -i</code>:</p> <pre><code>*/5 * * * * OWNER output=$(/usr/bin/cvmfs_server snapshot -a -i 2&gt;&amp;1) || echo \"$output\"\n</code></pre> <p>In here, you must replace \"<code>OWNER</code>\" with the account name of the repository owner (cfr. the <code>-o $USER</code> option used in the <code>add-replica</code> command above).</p> <p>The <code>-a</code> option enables synchronisation of all active replica repositories, while <code>-i</code> indicates that that repositories for which an initial snapshot has not been run should be skipped.</p> <p>To verify that periodic synchronisation is working correctly, check the contents of the log file:</p> <pre><code>/var/log/cvmfs/snapshots.log\n</code></pre>"},{"location":"access/stratum1/#more-information","title":"More information","text":"<p>For more information on the setup and configuration of a Stratum 1 replica server, see the CernVM-FS documentation, in particular the following sections:</p> <ul> <li>Notable CernVM-FS Server Locations and Files</li> <li>CernVM-FS Server Infrastructure</li> </ul>"},{"location":"access/stratum1/#using-the-private-stratum-1","title":"Using the private Stratum 1","text":"<p>To actually use the \"private\" Stratum 1 replica server that has been set up we need to change the configuration on each CernVM-FS client system.</p> <p>Initially, we will use only the private Stratum 1 replica server, without a proxy server.</p> <p>Remove <code>CVMFS_HTTP_PROXY</code> from client configuration</p> <p>Do make sure that the <code>CVMFS_HTTP_PROXY</code> line is removed from the CernVM-FS configuration file <code>/etc/cvmfs/default.local</code> on the client system, and that the CernVM-FS configuration was reloaded (with <code>sudo cvmfs_config reload</code>), as was instructed here.</p> <p>After we have verified that the Stratum 1 is used by the client system, we will bring the proxy server back in the game, and demonstrate how to use both the proxy server and the Stratum 1 replica server.</p>"},{"location":"access/stratum1/#only-private-stratum-1","title":"Only private Stratum 1","text":""},{"location":"access/stratum1/#client-configuration","title":"Client configuration","text":"<p>The <code>CVMFS_SERVER_URL</code> configuration setting on a client system:</p> <ul> <li>Is a string value with a semicolon-separated (<code>;</code>) list of known Stratum 1 servers;</li> <li>Should be enclosed in quotes;</li> <li>Specifies each Stratum 1 as a URL that starts with <code>http://</code>, and ends with <code>/cvmfs/@fqrn@</code></li> </ul> <p>For example:</p> <pre><code>CVMFS_SERVER_URL=\"http://s1.test.eu/cvmfs/@fqrn@;http://s1.test.us/cvmfs/@fqrn@\"\n</code></pre> <p>The <code>@fqrn@</code> substring is replaced by CernVM-FS with the fully qualified repository name, like <code>software.eessi.io</code>.</p> <p><code>CVMFS_SERVER_URL</code> should be specified in the domain-specific configuration file in <code>/etc/cvmfs</code> that is relevant for the CernVM-FS repository we have replicated on our Stratum 1.</p> <p>For <code>software.eessi.io</code>, we should add the following to <code>/etc/cvmfs/domain.d/eessi.io.local</code>:</p> <pre><code>CVMFS_SERVER_URL=\"http://STRATUM1_IP/cvmfs/@fqrn@\"\n</code></pre> <p>in which \"<code>STRATUM1_IP</code>\" must be replaced with (you guessed it) the IP address or hostname of the private Stratum 1 replica server.</p> <p>To apply the configuration change, run <code>cvmfs_config reload</code>:</p> <pre><code>sudo cvmfs_config reload\n</code></pre>"},{"location":"access/stratum1/#testing","title":"Testing","text":"<p>To verify that the client configuration was changed correctly, use <code>cvmfs_config stat</code> (which requires that the repository is mounted):</p> <pre><code>ls /cvmfs/software.eessi.io\ncvmfs_config stat -v software.eessi.io\n</code></pre> <p>The output line that starts with <code>Connection</code> should mention <code>online</code>, like this:</p> <pre><code>Connection: http://.../cvmfs/software.eessi.io through proxy DIRECT (online)\n</code></pre> <p>The <code>proxy DIRECT</code> indicates that we are not using a proxy server yet in this setup.</p> <p>You can also use <code>curl</code> to check the connection to the Stratum 1, by letting it print the HTTP header for the <code>.cvmfspublished</code> file in the root of the repository:</p> <pre><code>curl --head http://STRATUM1_IP/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> <p>the first line of the output should be something like: <pre><code>HTTP/1.1 200 OK\n</code></pre></p> <p>If instead you see <code>403 Forbidden</code> then the proxy server is blocking the connection: <pre><code>HTTP/1.1 403 Forbidden\n</code></pre></p>"},{"location":"access/stratum1/#proxy-private-stratum-1","title":"Proxy + private Stratum 1","text":"<p>To have a more complete view, let's now also bring the proxy server back in the game.</p>"},{"location":"access/stratum1/#reconfigure-squid-proxy","title":"Reconfigure Squid proxy","text":"<p>First we need to make a small but important change to the configuration of the Squid proxy, to ensure that the proxy server is allowed to connect to the private Stratum 1 replica server.</p> <p>Update the ACL for the Stratum 1 servers in <code>/etc/squid/squid.conf</code> on the proxy server by adding the IP address of the private Stratum 1: <pre><code># replace STRATUM1_IP with the IP address of the private Stratum 1\nacl stratum_ones dstdomain .eessi.science STRATUM1_IP\n</code></pre></p> <p>And then reload for configuration for the Squid proxy service:</p> <pre><code>sudo systemctl reload squid\n</code></pre>"},{"location":"access/stratum1/#client-configuration_1","title":"Client configuration","text":"<p>We also need to update the client configuration to restore the <code>CVMFS_HTTP_PROXY</code> line in <code>/etc/cvmfs/default.local</code>, like we did when using the proxy server:</p> <pre><code># replace PROXY_IP with the IP address of the proxy server\nCVMFS_HTTP_PROXY=\"http://PROXY_IP:3128\"\n</code></pre> <p>Don't forget to reload the CernVM-FS client configuration:</p> <pre><code>sudo cvmfs_config reload\n</code></pre>"},{"location":"access/stratum1/#testing_1","title":"Testing","text":"<p>To test whether the setup using both the proxy server and the Stratum 1 replica server works, we can try accessing the EESSI repository, for example by sourcing the initialisation script:</p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash\n</code></pre> <p>The output of <code>sudo cvmfs_config stat -v software.eessi.io</code> should include a <code>Connection</code> line that ends with <code>(online)</code>, like this:</p> <pre><code>Connection: http://STRATUM1_IP/cvmfs/software.eessi.io through proxy http://PROXY_IP:3128 (online)\n</code></pre> <p>You can also use <code>curl</code> to check whether the Stratum 1 can be reached via the proxy server:</p> <pre><code>http_proxy=http://PROXY_IP:3128 curl --head http://STRATUM1_IP/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre>"},{"location":"access/stratum1/#conclusions","title":"Conclusions","text":"<p>With a private Stratum 1 replica server, we have a more production-ready setup in place for using CernVM-FS.</p> <p>Using both a proxy server and a Stratum 1 replica server is another step in that direction, since it further improves the resilience, maintainability, scalability, and performance of the setup (since the proxy server can serve request from its memory cache).</p> <p>For the sake of demonstration we have used two separate systems for the Stratum 1 replica server and the proxy server, but both services can also be installed and configuration on the same server, and also installing multiple proxy servers is sensible to improve load balancing, for example to serve different HPC clusters that have significantly different workload mixes.</p> <p>(next: Alternative ways to access CernVM-FS repositories)</p>"},{"location":"appendix/terminology/","title":"CernVM-FS Terminology","text":"<p>An overview of terms used in the context of CernVM-FS, in alphabetical order.</p>"},{"location":"appendix/terminology/#catalog","title":"Catalog","text":"<p>A catalog of a CernVM-FS repository is a table that lists files and directories along with the corresponding metadata (sizes, timestamps, etc.).</p> <p>Catalogs can be nested: subtrees of the repository may have their own catalog.</p> <p>For more information on the catalog concept, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#cernvm","title":"CernVM","text":"<p>CernVM is a virtual machine image based on CentOS combined with a custom, virtualization-friendly Linux kernel, and which includes the CernVM-FS client software.</p> <p>It is used for the CERN Large Hadron Collider (LHC) experiment, and was created to remove a need for the installation of the experiment software and to minimize the number of platforms (compiler-OS combinations) on which experiment software needs to be supported and tested.</p> <p>While originally developed in conjunction, the CernVM File System today is a product that is completely independent from the CernVM virtual appliance.</p> <p>For more information on CernVM, see the website and documentation.</p>"},{"location":"appendix/terminology/#cvmfs","title":"CernVM-FS","text":"<p>(see What is CernVM-FS?)</p>"},{"location":"appendix/terminology/#client","title":"Client","text":"<p>A client in the context of CernVM-FS is a computer system on which a CernVM-FS repository is being accessed, on which it will be presented as a POSIX read-only file system in a subdirectory of <code>/cvmfs</code>.</p>"},{"location":"appendix/terminology/#proxy","title":"Proxy","text":"<p>A proxy, also referred to as squid proxy, is a forward caching proxy server which acts as an intermediary between a CernVM-FS client and the Stratum-1 replica servers.</p> <p>It is used to improve the latency observed when accessing the contents of a repository, and to reduce the load on the Stratum-1 replica servers.</p> <p>A commonly used proxy is Squid.</p> <p>For more information on proxies, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#publishing","title":"Publishing","text":"<p>Publishing is the process of adding more files to a CernVM-FS repository, which is done via a transaction mechanism, and is on possible on the Stratum-0 server, via a publisher, or via a repository gateway.</p> <p>The workflow of publishing content is covered in detail in the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#repository","title":"Repository","text":"<p>A CernVM-FS repository is where the files and directories that you want to distribute via CernVM-FS are stored, which usually correspond to a collection of software installations.</p> <p>It is a form of content-addressable storage (CAS), and is the single source of (new) data for the file system being presented as a subdirectory of <code>/cvmfs</code> on client systems that mount the repository.</p> <p>Note</p> <p>A CernVM-FS repository includes software installations, not software packages like RPMs.</p>"},{"location":"appendix/terminology/#software-installations","title":"Software installations","text":"<p>An important distinction for a CernVM-FS repository compared to the more traditional notion of a software repository is that a CernVM-FS repository provides access to the individual files that collectively form a particular software installation, as opposed to housing a set of software packages like RPMs, each of which being a collection of files for a particular software installation that are packed together in a single package to distribute as a whole.</p> <p>Note</p> <p>This is an important distinction, since CernVM-FS enables only downloading the specific files that are required to perform a particular task with a software installation, which often is a small subset of all files that are part of that software installation.</p>"},{"location":"appendix/terminology/#stratum0","title":"Stratum 0 server","text":"<p>A Stratum 0 server, often simply referred to a Stratum 0 (Stratum Zero), is the central server for one or more CernVM-FS repositories.</p> <p>It is the single source of (new) data, since it hosts the master copy of the repository contents.</p> <p>Adding or updating files in a CernVM-FS repository (publishing) can only be done on the Stratum 0 server, either directly via the <code>cvmfs_server publish</code> command, or indirectory via a publisher server.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#stratum1","title":"Stratum 1 replica server","text":"<p>A Stratum 1 replica server, often simply referred to a Stratum 1 (Stratum One), is a standard web server that acts as a mirror server for one or more CernVM-FS repositories.</p> <p>It holds a complete copy of the data for each CernVM-FS repository it serves, and automatically synchronises with the Stratum 0.</p> <p>There is typically a network of several Stratum 1 servers for a CernVM-FS repository, which are geographically distributed.</p> <p>Clients can be configured to automatically connect to the closest Stratum 1 server by using the CernVM-FS GeoAPI.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"cvmfs/","title":"Introduction to CernVM-FS","text":"<ul> <li>What is CernVM-FS?</li> <li>Technical details</li> <li>Flagship repositories</li> </ul>"},{"location":"cvmfs/flagship-repositories/","title":"Flagship CernVM-FS repositories","text":""},{"location":"cvmfs/flagship-repositories/#lhc-experiments","title":"LHC experiments","text":"<p>CernVM-FS repositories are used to distribute the software required to analyse the data produced by the Large Hadron Collider (LHC) at each of the LHC experiments.</p> <p>Examples include (click to browse repository contents):</p> <ul> <li><code>/cvmfs/alice.cern.ch</code>: software for ALICE experiment</li> <li><code>/cvmfs/atlas.cern.ch</code>: software for ATLAS experiment</li> <li><code>/cvmfs/cms.cern.ch</code>: software for CMS experiment</li> <li><code>/cvmfs/lhcb.cern.ch</code>: software for LHCb experiment</li> <li><code>/cvmfs/sft.cern.ch</code>: LCG Software Stacks</li> </ul>"},{"location":"cvmfs/flagship-repositories/#the-alliance","title":"The Alliance","text":"<p>The Digital Research Alliance of Canada, a.k.a. The Alliance and formerly known as Compute Canada, uses CernVM-FS to distribute the software stack for the Canadian national compute clusters.</p> <p>Documentation on using their CernVM-FS repository <code>/cvmfs/soft.computecanada.ca</code> is available here, and an overview of available software is available here.</p>"},{"location":"cvmfs/flagship-repositories/#unpacked-containers","title":"Unpacked containers","text":"<p>CernVM-FS repositories can be used to provide an efficient way to access container images, by serving unpacked container images that can be consumed by container runtimes such as Apptainer.</p> <p>Examples include:</p> <ul> <li><code>/cvmfs/unpacked.cern.ch</code></li> <li><code>/cvmfs/singularity.opensciencegrid.org</code></li> </ul> <p>More information on <code>unpacked.cern.ch</code> is available in the CernVM-FS documentation:</p> <ul> <li>Container Images and CernVM-FS</li> <li>Working with DUCC and Docker Images</li> </ul>"},{"location":"cvmfs/flagship-repositories/#eessi","title":"EESSI","text":"<p>The European Environment for Scientific Software Installations (EESSI) provides optimized installations of scientific software for <code>x86_64</code> (Intel + AMD) and <code>aarch64</code> (64-bit Arm) systems that work on any Linux distribution.</p> <p>We will use EESSI as an example CernVM-FS repository throughout this tutorial.</p> <p>(next: What is EESSI?)</p>"},{"location":"cvmfs/technical-details/","title":"Technical details of CernVM-FS","text":"<p>CernVM-FS is implemented as a POSIX read-only filesystem in user space (FUSE) with repositories of files that are served via outgoing HTTP connections only, thus avoiding problems with firewalls.</p> <p>Files in a CernVM-FS repository are automatically downloaded on-demand to a client system as they are accessed, from web servers that support the CernVM-FS repository being used.</p> <p>Internally, CernVM-FS uses content-adressable storage (CAS) and Merkle trees (like Git also does) to store file data and metadata.</p>"},{"location":"cvmfs/technical-details/#caching","title":"Caching","text":"<p>CernVM-FS uses a caching mechanism with a least-recently used (LRU) cache replacement policy, in which configurable local client cache is populated via either a forward proxy server (like Squid), or from a Stratum-1 replica server.</p> <p>Both the proxy and the replica server could be within the same local network as the client, or not.</p> <p>To help reduce performance problems regarding network latency and bandwidth, clients can leverage the Geo API supported by CernVM-FS Stratum-1 replica servers to automatically sort them geographically, in order to prioritize connecting to the closest ones.</p> <p>Furthermore, additional caches can be made available to CernVM-FS, such as an alien cache on a shared cluster filesystem like GPFS or Lustre that is not managed by CernVM-FS, and a Content Delivery Network (CDN) can be used to help limit the time required to download files that are not cached yet.</p> <p>(next: Flagship CernVM-FS repositories)</p>"},{"location":"cvmfs/what-is-cvmfs/","title":"What is CernVM-FS?","text":"<p>CernVM-FS, the CernVM File System (also known as CVMFS), is a file distribution service that is particularly well suited to distribute software installations across a large number of systems world-wide in an efficient way.</p> <p>From an end user perspective, files in a CernVM-FS repository are available read-only via a subdirectory in <code>/cvmfs</code>, with a user experience similar to that of an on-demand streaming service for music or video, but then (mainly) applied to software installations.</p>"},{"location":"cvmfs/what-is-cvmfs/#primary-use-case","title":"Primary use case","text":"<p>The primary use case of CernVM-FS is distributing software, and it provides several interesting features that support this, including:</p> <ul> <li>on-demand downloading and updating of repository contents;</li> <li>multi-level caching;</li> <li>de-duplication of files;</li> <li>compression of data;</li> <li>verification of data integrity;</li> </ul> <p>CernVM-FS has been proven to scale to billions of files and tens of thousands of clients.</p> <p>It was originally developed at CERN to let High Energy Physics (HEP) collaborations like the experiments at the Large Hadron Collider (LHC) deploy software on the Worldwide LHC Computing Grid (WLCG) infrastructure that is used to run data processing applications.</p> <p>The primary use case of distributing software is a particular one, since software often comprises many small files that are frequently opened and read as a whole, and frequent look-ups for files in multiple directories are triggered when search paths are examined.</p> <p>In certain cases, the CernVM-FS has also been used to distribute large data repositories.</p>"},{"location":"cvmfs/what-is-cvmfs/#features","title":"Features","text":""},{"location":"cvmfs/what-is-cvmfs/#features-ondemand","title":"On-demand downloading of files and metadata","text":"<p>The metadata and content of files included in a CernVM-FS repository are automatically downloaded on-demand as files and directories are being accessed, which is akin to streaming services for music, movies, and TV series.</p> <p>This happens fully transparently, as the contents of a repository are exposed by CernVM-FS as if it were a local (read-only) file system. Hence, clients that access a CernVM-FS repository typically do not actually have a local copy of all files included in that repository, but only have a limited set of files and metadata directly available: those which were most recently accessed.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-updating","title":"Automatic updates","text":"<p>CernVM-FS clients automatically pull in updates to the contents of a repository as they are published server-side. This happens in transactions, to ensure that clients observe a consistent state of the repository.</p> <p>Once a CernVM-FS repository is accessible on a client system, no subsequent actions must be taken to keep clients up-to-date other than updating CernVM-FS itself on a regular basis.</p> <p>This significantly limits the maintenance burden, since no action is required on client systems to update the software stack that is provided through a CernVM-FS repository, since the updates are streamed in automatically by CernVM-FS.</p> <p>Only the CernVM-FS client should be updated on a regular basis on client systems.</p> <p>For more elaborate setups that involve proxies or CernVM-FS replica (mirror) servers, additional maintenance is necessary, but again only to update the CernVM-FS components themselves.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-caching","title":"Multi-level caching","text":"<p>CernVM-FS uses a multi-level caching hierarchy to reduce the latency observed when accessing repository contents. Caching is an essential part of CernVM-FS, since the contents of a CernVM-FS repository are downloaded on-demand as they are accessed.</p> <p>The caching mechanism employed by CernVM-FS goes way beyond the standard (in-memory) Linux kernel file system cache, and consists of a local client cache, an optional forward proxy server that acts as an intermediary cache level, and a distributed network of mirror servers that support the CernVM-FS repository being accessed.</p> <p>When a part of the repository is being accessed that is not available yet in the local client cache, CernVM-FS will traverse the multi-level cache hierarchy to obtain the necessary data and update the local client cache with it, so the files being accessed can be served with low latency.</p> <p>Proxy and mirror servers scale horizontally: the CernVM-FS client makes automatic use of multiple deployed service instances for load-balancing and high-availability.</p> <p>We will explore this multi-level caching mechanism in more detail in this tutorial.</p> <p>See here more technical details on CernVM-FS caching.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-deduplication","title":"De-duplication of files","text":"<p>CernVM-FS stores the contents of a file only once, even when it is included multiple times in a particular repository at different paths.</p> <p>This can result in a significant reduction in storage capacity that is required to host a large software stack, especially when identical files are spread out across the repository, as often happens with particular files like example data files across multiple versions of the same software.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-compression","title":"Compression of data","text":"<p>CernVM-FS stores file content compressed on the server, which not only further reduces required storage space but also significantly limits the network bandwidth that is required to download (and serve) the contents of a repository.</p> <p>On the client side, the data is transparently decompressed when the files included in a CernVM-FS repository are presented under <code>/cvmfs</code> as a normal (read-only) file system.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-data-integrity","title":"Verification of data integrity","text":"<p>The integrity of data provided by a CernVM-FS server is ensured on a client system by verifying a cryptographic hash, which is again a direct result of content-addressable storage mechanism that is used by CernVM-FS. This is an essential security aspect since CernVM-FS uses (possibly untrusted) caches and HTTP connections to distribute the contents of a repository.</p> <p>(next: Technical details of CernVM-FS)</p>"},{"location":"eessi/","title":"EESSI","text":""},{"location":"eessi/#european-environment-for-scientific-software-installations","title":"European Environment for Scientific Software Installations","text":"<ul> <li>What is EESSI?</li> <li>Motivation &amp; Goals</li> <li>Inspiration</li> <li>High-level design</li> <li>Using EESSI</li> </ul>"},{"location":"eessi/high-level-design/","title":"High-level design of EESSI","text":"<p>The design of EESSI is very similar to that of the Compute Canada software stack it is inspired by, and is aligned with the motivation and goals of the project.</p> <p>In the remainder of this section of the tutorial, we will explore the layered structure of the EESSI software stack, and how to use it.</p> <p>In the next section will cover in detail how you can get access to EESSI (and other publicly available CernVM-FS repositories).</p>"},{"location":"eessi/high-level-design/#layered-structure","title":"Layered structure","text":"<p>The EESSI project consists of 3 layers, which are constructed by leveraging various open source software projects.</p> <p> </p>"},{"location":"eessi/high-level-design/#filesystem_layer","title":"Filesystem layer","text":"<p>The filesystem layer is responsible for distributing the EESSI software stack to systems on which is it used.</p> <p>This is done using CernVM-FS, which is a mature open source software project that was created exactly for this purpose: to distribute software installations worldwide reliably and efficiently in a scalable way. As such, it aligns very well with the goals of EESSI.</p> <p>The CernVM-FS repository for EESSI is <code>/cvmfs/software.eessi.io</code>, which is part of the default CernVM-FS configuration since 21 November 2023, so no additional action is required to gain access to it other than installing and configuration the client component of CernVM-FS.</p> <p>More on that in the next section of this tutorial.</p> Note on the EESSI pilot repository (click to expand) <p>There is also a \"pilot\" CernVM-FS repository for EESSI (<code>/cvmfs/pilot.eessi-hpc.org</code>), which was primarily used to gain experience with CernVM-FS in the early years of the EESSI project.</p> <p>Although it is still available currently, we do not recommend using it.</p> <p>Not only will you need to install the CernVM-FS configuration for EESSI to gain access to it, there also are no guarantees that the EESSI pilot repository will remain stable or even available, nor that the software installations it provides are actually functional, since it may be used for experimentation purposes by the EESSI maintainers.</p>"},{"location":"eessi/high-level-design/#compat_layer","title":"Compatibility layer","text":"<p>The compatibility layer of EESSI levels the ground across different (versions of) the Linux operating system (OS) of client systems that use the software installations provided by EESSI.</p> <p>It consists of a limited set of libraries and tools that are installed in a non-standard filesystem location (a \"prefix\"), which were built from source for the supported CPU families using Gentoo Prefix.</p> <p>The installation path of the EESSI compatibility layer corresponds to the <code>compat</code> subdirectory of a specific version of EESSI (like <code>2023.06</code>) in the EESSI CernVM-FS repository, which is specific to a particular type of OS (currently only <code>linux</code>) and CPU family (currently <code>x86_64</code> and <code>aarch64</code>):</p> <pre><code>$ ls /cvmfs/software.eessi.io/versions/2023.06/compat\nlinux\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux\naarch64  x86_64\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64\nbin  etc  lib  lib64  opt  reprod  run  sbin  stage1.log  stage2.log  stage3.log  startprefix  tmp  usr  var\n\n$ ls -l /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64\ntotal 4923\n-rwxr-xr-x 1 cvmfs cvmfs  210528 Nov 15 11:22 ld-linux-x86-64.so.2\n...\n-rwxr-xr-x 1 cvmfs cvmfs 1876824 Nov 15 11:22 libc.so.6\n...\n-rwxr-xr-x 1 cvmfs cvmfs  911600 Nov 15 11:22 libm.so.6\n...\n</code></pre> <p>Libraries included in the compatibility layer can be used on any Linux client system, as long as the CPU family is compatible and taken into account.</p> <pre><code>$ uname -m\nx86_64\n\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux release 8.8 (Ootpa)\n\n$ /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64/libc.so.6\nGNU C Library (Gentoo 2.37-r7 (patchset 10)) stable release version 2.37.\n...\n</code></pre> <p>By making sure that the software installations included in EESSI only rely on tools and libraries provided by the compatibility layer, and do not (directly) require anything from the client OS, we can ensure that they can be used in a broad variety of Linux systems, regardless of the (version of) Linux distribution being used.</p> <p>Note</p> <p>This is very similar to the OS tools and libraries that are included in container images, except that no container runtime is involved here (typically), only CernVM-FS.</p>"},{"location":"eessi/high-level-design/#software_layer","title":"Software layer","text":"<p>The top layer of EESSI is called the software layer, which contains the actual scientific software applications and their dependencies.</p>"},{"location":"eessi/high-level-design/#easybuild","title":"EasyBuild to install software","text":"<p>Building, managing, and optimising the software installations included in the software layer is layer is done using EasyBuild, a well-established software build and installation framework for managing (scientific) software stacks on High-Performance Computing (HPC) systems.</p>"},{"location":"eessi/high-level-design/#lmod","title":"Lmod as user interface","text":"<p>Next to installing the software itself, EasyBuild also automatically generates environment module files. These files, which are essentially small Lua scripts, are consumed via Lmod, a modern implementation of the concept of environment modules which provides a user-friendly interface to end users of EESSI.</p>"},{"location":"eessi/high-level-design/#cpu_detection","title":"CPU detection via <code>archspec</code> or <code>archdetect</code>","text":"<p>The initialisation script that is included in the EESSI repository automatically detects the CPU family and microarchitecture of a client system by leveraging either <code>archspec</code>, a small Python library, or <code>archdetect</code>, a minimal pure bash implementation of the same concept.</p> <p>Based on the features of the detected CPU microarchitecture, the EESSI initialisation script will automatically select the best suited subdirectory of the software layer that contains software installations that are optimised for that particular type of CPU, and update the session environment to start using it.</p>"},{"location":"eessi/high-level-design/#software_layer_structure","title":"Structure of the software layer","text":"<p>For now, we just briefly show the structure of <code>software</code> subdirectory that contains the software layer of a particular version of EESSI below.</p> <p>The <code>software</code> subdirectory is located at the same level as the <code>compat</code> directory for a particular version of EESSI, along with the <code>init</code> subdirectory that provides initialisation scripts:</p> <pre><code>$ cd /cvmfs/software.eessi.io/versions/2023.06\n$ ls\ncompat  init  software\n</code></pre> <p>In the <code>software</code> subdirectory, a subtree of directories is located that contains software installations that are specific to a particular OS family (only <code>linux</code> currently) and a specific CPU microarchitecture (with <code>generic</code> as a fallback):</p> <pre><code>$ ls software\nlinux\n\n$ ls software/linux\naarch64  x86_64\n\n$ ls software/linux/aarch64\ngeneric  neoverse_n1  neoverse_v1\n\n$ ls software/linux/x86_64\namd  generic  intel\n\n$ ls software/linux/x86_64/amd\nzen2  zen3\n\n$ ls software/linux/x86_64/intel\nhaswell  skylake_avx512\n</code></pre> <p>Each subdirectory that is specific to a particular CPU microarchitecure provides the actual optimised software installations (in <code>software</code>) and environment module files (in <code>modules/all</code>).</p> <p>Here we explore the path that is specific to AMD Milan CPUs, which have the Zen3 microarchitecture, focusing on the installations of OpenBLAS:</p> <pre><code>$ ls software/linux/x86_64/amd/zen3\nmodules  software\n\n$ ls software/linux/x86_64/amd/zen3/software\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/\n0.3.21-GCC-12.2.0  0.3.23-GCC-12.3.0\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/\nbin  easybuild  include  lib  lib64\n\n$ ls software/linux/x86_64/amd/zen3/modules/all\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/modules/all/OpenBLAS\n0.3.21-GCC-12.2.0.lua  0.3.23-GCC-12.3.0.lua\n</code></pre> <p>Each of the other subdirectories for specific CPU microarchitectures will have the exact same structure, and provide the same software installations and accompanying environment module files to access them with Lmod.</p> <p>A key aspect here is that binaries and libraries that make part of the software installations included in the EESSI software layer only rely on libraries provided by the compatibility layer and/or other software installations in the EESSI software layer.</p> <p>See for example libraries to which the OpenBLAS library links:</p> <pre><code>$ ldd software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/lib/libopenblas.so\n    linux-vdso.so.1 (0x00007ffd4373d000)\n    libm.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libm.so.6 (0x000014d0884c8000)\n    libgfortran.so.5 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgfortran.so.5 (0x000014d087115000)\n    libgomp.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgomp.so.1 (0x000014d088480000)\n    libc.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libc.so.6 (0x000014d086f43000)\n    /lib64/ld-linux-x86-64.so.2 (0x000014d08837e000)\n    libpthread.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libpthread.so.0 (0x000014d088479000)\n    libdl.so.2 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libdl.so.2 (0x000014d088474000)\n    libquadmath.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libquadmath.so.0 (0x000014d08842d000)\n    libgcc_s.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgcc_s.so.1 (0x000014d08840d000)\n</code></pre> Note on <code>/lib64/ld-linux-x86-64.so.2</code> (click to expand) <p>The <code>/lib64/ld-linux-x86-64.so.2</code> path, which corresponds to the dynamic linker/loader of the Linux client OS, that is shown in the output of <code>ldd</code> above is a bit misleading.</p> <p>It only pops up because we are running the <code>ldd</code> command provided by the client OS, which typically resides at <code>/usr/bin/ldd</code>.</p> <p>When actually running software provided by the EESSI software layer, the loader provided by the EESSI compatibility layer is used to launch binaries.</p> <p>We will explore the EESSI software layer a bit more in the next subsection of this tutorial, when we demonstrate how to use the software installations provided in the EESSI software layer.</p> <p>(next: Using EESSI)</p>"},{"location":"eessi/inspiration/","title":"Inspiration for EESSI","text":"<p>The EESSI concept is heavily inspired by software stack provided by the Digital Research Alliance of Canada (a.k.a. The Alliance, formerly known as Compute Canada), which is a shared software stack used on all national host sites for Advanced Research Computing in Canada that is distributed across Canada (and beyond) using CernVM-FS; see also here.</p> <p>EESSI is significantly more ambitious in its goals however, in various ways.</p> <p>It intends to support a broader range of system architectures than what is currently supported by the Compute Canada software stack, like Arm 64-bit microprocessors, accelerators beyond NVIDIA GPUs, etc.</p> <p>In addition, EESSI is set up to be a community project, by setting up services and infrastructure to automate the software build and installation process as much as possible, providing extensive documentation and support to end users, user support teams, and system administrators who want to employ EESSI, and allowing contributors to propose additions to the software stack.</p> <p>The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\".</p> <p>It has also been presented at the 5th EasyBuild User Meeting, see slides and talk recording.</p> <p>More information on the Compute Canada software stack is available in their documentation, and in their overview of available software.</p> <p>(next: High-level Overview of EESSI)</p>"},{"location":"eessi/motivation-goals/","title":"Motivation &amp; Goals of EESSI","text":""},{"location":"eessi/motivation-goals/#motivation","title":"Motivation","text":"<p>EESSI is motivated by the observation that the landscape of computational science is changing in various ways, including:</p> <ul> <li>Increasing diversity in system architectures: additional families of general-purpose   microprocessors including Arm 64-bit (<code>aarch64</code>) and   RISC-V on top of the well-established Intel and AMD processors (both <code>x86_64</code>),   and different types of GPUS (NVIDIA, AMD, Intel);</li> <li>Rapid expansion of computational science beyond traditional domains like physics and computational chemistry,   including bioinformatis, Machine Learning (ML) and Artificial Intelligence (AI), etc.,   which leads to a significant growth of the software stack that is used for running scientific workloads;</li> <li>Emergence of commercial cloud infrastructure (Amazon EC2,   Microsoft Azure, ...)   that has competitive advantages over on-premise infrastructure for computational workloads, such as near-instant   availability, increased flexibility, a broader variety of hardware platforms, and faster access to   new generations of microprocessors;</li> <li>Limited manpower that is available in the HPC user support teams that are responsible for helping   scientists with running the software they require on high-end (and complex) infrastructure like supercomputers   (and beyond);</li> </ul> <p>Collectively, these indicate that there is a strong need for more collaboration on building and installing scientific software to avoid duplicate work across computational scientists and HPC user support teams.</p>"},{"location":"eessi/motivation-goals/#goals","title":"Goals","text":"<p>The main goal of EESSI is to provide a collection of scientific software installations that work across a wide range of different platforms, including HPC clusters, cloud infrastructure, and personal workstations and laptops, without making compromises on the performance of that software.</p> <p>While initially the focus of EESSI is to support Linux systems with established system architectures like AMD + Intel CPUs and NVIDIA GPUs, the ambition is to also cover emerging technologies like Arm 64-bit CPUs, other accelerators like the AMD Instinct and Intel Xe, and eventually also RISC-V microprocessors.</p> <p>The software installations included in EESSI are optimized for specific generations of microprocessors by targeting a variety of instruction set architectures (ISAs), like for example Intel and AMD processors supporting the AVX2 or AVX-512 instructions, and Arm processors that support SVE instructions.</p> <p>(next: Inspiration for EESSI)</p>"},{"location":"eessi/support/","title":"Getting support for EESSI","text":"<p>Thanks to the funding provided by the MultiXscale EuroHPC JU Centre-of-Excellence, a dedicated support team is available to provide help on accessing or using EESSI.</p> <p>If you have any questions, or if you are experiencing problems, do not hesitate to reach out by either opening an issue in the EESSI support portal, or sending an email to <code>support@eessi.io</code>.</p> <p>For more information, see the support section of the EESSI documentation.</p> <p>(next: CernVM-FS client system)</p>"},{"location":"eessi/using-eessi/","title":"Using EESSI","text":"<p>Using the software installations provided by the EESSI CernVM-FS repository <code>software.eessi.io</code> is fairly straightforward.</p> <p>Let's break it down step by step.</p>"},{"location":"eessi/using-eessi/#0-is-eessi-available","title":"0) Is EESSI available?","text":"<p>First, check whether the EESSI CernVM-FS repository is available on your system.</p> <p>Try checking the contents of the <code>/cvmfs/software.eessi.io</code> directory with the <code>ls</code> command:</p> <pre><code>$ ls /cvmfs/software.eessi.io\nhost_injections  README  versions\n</code></pre> <p>If you see an error message like \"<code>No such file or directory</code>\", then either the CernVM-FS client is not installed on your system, or the configuration for the EESSI repository is not available. In that case, you may want to revisit the Accessing a CernVM-FS repository section, or go through the Troubleshooting section.</p> Don't be fooled by <code>autofs</code> (click to expand) <p>The <code>/cvmfs</code> directory may seem empty at first, because CernVM-FS repositories are automatically mounted as they are accessed via <code>autofs</code>.</p> <p>So rather than just using \"<code>ls /cvmfs/</code>\" to check which CernVM-FS repositories are available on your system, you should try to directly access a specific repository as shown above for EESSI with <code>ls /cvmfs/software.eessi.io</code> .</p> <p>For more information on various aspects of mounting of CernVM-FS repositories, see the CernVM-FS documentation.</p>"},{"location":"eessi/using-eessi/#init","title":"1) Initialise shell environment","text":"<p>If the EESSI repository is available, you can proceed to preparing your shell environment for using a particular version of EESSI by sourcing the provided initialisation script by running the <code>source</code> command:</p> <pre><code>$ source /cvmfs/software.eessi.io/versions/2023.06/init/bash\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n</code></pre> Details on changes made to the shell environment (click to expand) <p>The initialisation script is a simple bash script that changes a couple of environment variables:</p> <ul> <li>A set of <code>$EESSI_*</code> environment variables is defined;</li> <li>The <code>$PS1</code> environment variable that specifies the shell prompt   is updated to indicate that your shell session has been initialised for EESSI;</li> <li>The location of the tools provided by the EESSI compatibility layer are prepended to the <code>$PATH</code> environment variable;</li> <li>Lmod, which is included in the EESSI compatibility layer, is initialised to ensure that the <code>module</code> command is defined,   and that the Lmod spider cache that is included in the EESSI software layer is picked up;</li> <li>The location to the software installations that are optimised for the CPU microarchitecture of the client system   is prepended to the <code>$MODULEPATH</code> environment variable by running a \"<code>module use</code>\" command.</li> </ul> <p>Note how the CPU microarchitecture is being auto-detected, which determines which path that points to a set of environment module files is used to update <code>$MODULEPATH</code>.</p> <p>This ensures that the modules that will be loaded provide access to software installations from the EESSI software layer that are optimised for the system you are using EESSI on.</p>"},{"location":"eessi/using-eessi/#2-load-modules","title":"2) Load module(s)","text":"<p>After initialising your shell environment for using EESSI, you can start exploring the EESSI software layer using the <code>module</code> command.</p> <p>Using <code>module avail</code> (or <code>ml av</code>), you can check which software is available. Without extra arguments, <code>module avail</code> will produce an overview of all available software. By passing an extra argument you can filter the results and search for specific software:</p> <pre><code>$ module avail tensorflow\n\n----- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all -----\n\n    TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>To start using software you should load the corresponding environment module files using <code>module load</code> (or <code>ml</code>). For example:</p> <pre><code>$ module load TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>A <code>module load</code> command usually does not produce any output, but it updates your shell environment to make the software ready to use.</p> <p>For more information on the <code>module</code> command, see the User Guide for Lmod.</p>"},{"location":"eessi/using-eessi/#3-use-software","title":"3) Use software","text":"<p>After loading a module, you should be able to use the corresponding software.</p> <p>For example, after loading the <code>TensorFlow/2.13.0-foss-2023a</code> module, you can start a Python session and play with the <code>tensorflow</code> Python package:</p> <pre><code>$ python\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.13.0'\n</code></pre> <p>Keep in mind that you are using a Python installation provided by the EESSI software layer here, not the Python version that may be provided by your client OS:</p> <pre><code>$ command -v python\n/cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/software/Python/3.11.3-GCCcore-12.3.0/bin/python\n</code></pre> Initial start-up delay (click to expand) <p>You may notice a bit of \"lag\" initially when starting to use software provided by the EESSI software layer.</p> <p>This is expected, since CernVM-FS may need to first download the files that are required to run the software you are using; see also On-demand downloading of files and metadata.</p> <p>You should not observe any significant start-up delays anymore when running the same software shortly after, since then CernVM-FS will be able to serve the necessary files from the local client cache; see also Multi-level caching.</p> <p>(next: Getting support for EESSI)</p>"},{"location":"eessi/what-is-eessi/","title":"What is EESSI?","text":"<p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in the HPC (High Performance Computing) community.</p> <p>EESSI provides a common stack of optimized scientific software installations that work on any Linux distribution, and currently supports both <code>x86_64</code> (AMD/Intel) and <code>aarch64</code> (Arm 64-bit) systems, which is distributed via CernVM-FS.</p> <p>(next: Motivation &amp; Goals of EESSI)</p>"}]}